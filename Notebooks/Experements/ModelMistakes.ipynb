{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ModelMistakes.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TZLgnaoJ9OS",
        "colab_type": "code",
        "outputId": "98e02a91-9123-4e9d-c1d3-70e8b84a572d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpNXJwI5KCEJ",
        "colab_type": "code",
        "outputId": "28679b85-5b66-4009-f74d-a1f7b0b96469",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "source": [
        "!cp -avr /content/drive/'My Drive'/DetectingSarcasm ."
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'/content/drive/My Drive/DetectingSarcasm' -> './DetectingSarcasm'\n",
            "'/content/drive/My Drive/DetectingSarcasm/Models' -> './DetectingSarcasm/Models'\n",
            "'/content/drive/My Drive/DetectingSarcasm/Models/LSTM2DMaxPool.py' -> './DetectingSarcasm/Models/LSTM2DMaxPool.py'\n",
            "'/content/drive/My Drive/DetectingSarcasm/Models/AttantionLSTM.py' -> './DetectingSarcasm/Models/AttantionLSTM.py'\n",
            "'/content/drive/My Drive/DetectingSarcasm/Models/BidirectionalLSTM.py' -> './DetectingSarcasm/Models/BidirectionalLSTM.py'\n",
            "'/content/drive/My Drive/DetectingSarcasm/Models/LSTM.py' -> './DetectingSarcasm/Models/LSTM.py'\n",
            "'/content/drive/My Drive/DetectingSarcasm/Models/Weights' -> './DetectingSarcasm/Models/Weights'\n",
            "'/content/drive/My Drive/DetectingSarcasm/Notebooks' -> './DetectingSarcasm/Notebooks'\n",
            "'/content/drive/My Drive/DetectingSarcasm/Notebooks/LSTMs.ipynb' -> './DetectingSarcasm/Notebooks/LSTMs.ipynb'\n",
            "'/content/drive/My Drive/DetectingSarcasm/Notebooks/BERT.ipynb' -> './DetectingSarcasm/Notebooks/BERT.ipynb'\n",
            "'/content/drive/My Drive/DetectingSarcasm/Notebooks/Weights' -> './DetectingSarcasm/Notebooks/Weights'\n",
            "'/content/drive/My Drive/DetectingSarcasm/Notebooks/Experements' -> './DetectingSarcasm/Notebooks/Experements'\n",
            "'/content/drive/My Drive/DetectingSarcasm/Notebooks/Experements/TuneLSTMs.ipynb' -> './DetectingSarcasm/Notebooks/Experements/TuneLSTMs.ipynb'\n",
            "'/content/drive/My Drive/DetectingSarcasm/Notebooks/Experements/CleanedDataLSTMs.ipynb' -> './DetectingSarcasm/Notebooks/Experements/CleanedDataLSTMs.ipynb'\n",
            "'/content/drive/My Drive/DetectingSarcasm/Data' -> './DetectingSarcasm/Data'\n",
            "'/content/drive/My Drive/DetectingSarcasm/Data/Sarcasm_Headlines_Dataset_v2.json' -> './DetectingSarcasm/Data/Sarcasm_Headlines_Dataset_v2.json'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1m7ISXQKPm1",
        "colab_type": "code",
        "outputId": "d7c50ef8-8f1c-4c1e-b0bd-6f9a45804531",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd DetectingSarcasm/Notebooks/Experements"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/DetectingSarcasm/Notebooks/Experements\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcL5kYXzKTk-",
        "colab_type": "code",
        "outputId": "e5cdf7aa-948f-49a6-b3d3-123841f08eb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CleanedDataLSTMs.ipynb\tTuneLSTMs.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeQV9fvC7sg9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('../../Models')\n",
        "# sys.path.append('../')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Q0B2v8U4Sq7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torchtext import data\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import spacy\n",
        "\n",
        "import time\n",
        "import copy\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "from torchtext.vocab import Vectors, GloVe\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXiJAQcG4wRK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_json(\"../../Data/Sarcasm_Headlines_Dataset_v2.json\", lines=True)\n",
        "df = df.drop(['article_link'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3k_QAjY8NfS",
        "colab_type": "text"
      },
      "source": [
        "We will use 20% of dataset as test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1I2Efz127_x9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = df['headline']\n",
        "y = df['is_sarcastic']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVLKjX6s8mt6",
        "colab_type": "text"
      },
      "source": [
        "Loading test and train sets to csv:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbIDzEg88hh3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir torchtext_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KFBKn9T8c9B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = pd.concat([X_train, y_train], axis=1)\n",
        "test_data = pd.concat([X_test, y_test], axis=1)\n",
        "\n",
        "train_data.to_csv(\"torchtext_data/train.csv\", index=False)\n",
        "test_data.to_csv(\"torchtext_data/test.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T29G95Ve-T7a",
        "colab_type": "text"
      },
      "source": [
        "Prepearing data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcjqovOR9hvZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJGcqHhW84QM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fix_length = 24\n",
        "TEXT = data.Field(sequential=True, tokenize=\"spacy\", fix_length=fix_length)\n",
        "LABEL = data.LabelField(dtype=torch.long, sequential=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-01tlYZ-Mn1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data, test_data = data.TabularDataset.splits(\n",
        "    path=\"torchtext_data/\", train=\"train.csv\", \n",
        "    test=\"test.csv\",format=\"csv\", skip_header=True, \n",
        "    fields=[('Text', TEXT), ('Label', LABEL)]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwhDW7py_VU1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 16\n",
        "\n",
        "train_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, test_data), sort_key=lambda x: len(x.Text),\n",
        "    batch_size=batch_size,\n",
        "    device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3woO5khX-rjD",
        "colab_type": "text"
      },
      "source": [
        "Making vocabluary with GloVe with dimantion of 300:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pZltSV3-nQ5",
        "colab_type": "code",
        "outputId": "bc2baccf-10cf-448b-bc79-0a8446cbdad8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "TEXT.build_vocab(train_data, vectors=GloVe(name='6B', dim=300))\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [06:31, 2.20MB/s]                           \n",
            "100%|█████████▉| 399207/400000 [00:36<00:00, 11212.17it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDLwbpyU_osm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_embeddings = TEXT.vocab.vectors\n",
        "output_size = 2 #two classes\n",
        "num_layers = 1\n",
        "hidden_size = 128 #num of units in our NN\n",
        "embedding_length = 300 #dim of GloVe vector\n",
        "vocab_size = len(TEXT.vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ls7JtFSAHn6",
        "colab_type": "text"
      },
      "source": [
        "Defining the model:\n",
        "\n",
        "\n",
        "*   Simple LSTM - LSTMClassifier class \n",
        "*   LSTM with Attanrion - AttantionLSTMClassifier class \n",
        "*   Bidirectional LSTM - BidirectionalLSTMClassifier class\n",
        "*   Bidirectional LSTM with 2D MaxPool layer - LSTM2DMaxPoolClassifier class\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWyTUxgj_HgM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from LSTM2DMaxPool import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XIX2fK7AFKW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = LSTM2DMaxPoolClassifier(batch_size, output_size, hidden_size, vocab_size, embedding_length, num_layers, word_embeddings)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiv6zysuCgtT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.000174282, weight_decay=4.62355e-05)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQPqdzMnCp1c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1977cba2-33c3-4421-821a-af7f3ebf5203"
      },
      "source": [
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r100%|█████████▉| 399207/400000 [00:50<00:00, 11212.17it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5xutdQXCwsH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataiter_dict = {'train': train_iterator, 'val': test_iterator}\n",
        "dataset_sizes = {'train':len(train_data), 'val':len(test_data)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYFOBmacC5fO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "    print('starting')\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss = 200\n",
        "\n",
        "    val_loss = []\n",
        "    train_loss = []\n",
        "    val_acc = []\n",
        "    train_acc = []\n",
        "\n",
        "    uncorrect_texts = []\n",
        "    correct_labels = np.array([])\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                #scheduler.step()\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            sentiment_corrects = 0\n",
        "            tp = 0.0 # true positive\n",
        "            tn = 0.0 # true negative\n",
        "            fp = 0.0 # false positive\n",
        "            fn = 0.0 # false negative\n",
        "\n",
        "            # Iterate over data.\n",
        "            for batch in dataiter_dict[phase]:\n",
        "                \n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    text = batch.Text\n",
        "                    label = batch.Label\n",
        "                    label = torch.autograd.Variable(label).long()\n",
        "\n",
        "                    if torch.cuda.is_available():\n",
        "                      text = text.cuda()\n",
        "                      label = label.cuda()\n",
        "                    if (batch.Text.size()[1] is not batch_size):\n",
        "                      continue\n",
        "                    \n",
        "                    outputs = model(text)\n",
        "                    outputs = F.softmax(outputs,dim=-1)                   \n",
        "                    loss = criterion(outputs, label)\n",
        "\n",
        "                    if phase == 'val' and epoch == num_epochs - 1:\n",
        "                      itos = np.array(TEXT.vocab.itos) #getting indexes to words\n",
        "                      mask = torch.max(outputs, 1)[1] != label #finding places where model made mistake\n",
        "                      uncorrect_idxs = np.nonzero(mask).reshape(-1) #finding corresponding indexes\n",
        "                      uncorrect_text = itos[text[:,uncorrect_idxs].cpu()].T #getting corresponding text\n",
        "                      uncorrect_texts.append(uncorrect_text)\n",
        "                      labels = np.array(label[mask].cpu()) #finding true label, where model made mistakes\n",
        "                      correct_labels = np.append(correct_labels, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        \n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * text.size(0)\n",
        "                sentiment_corrects += torch.sum(torch.max(outputs, 1)[1] == label)\n",
        "\n",
        "                tp += torch.sum(torch.max(outputs, 1)[1] & label)\n",
        "                tn += torch.sum(1-torch.max(outputs, 1)[1] & 1-label)\n",
        "                fp += torch.sum(torch.max(outputs, 1)[1] & 1-label)\n",
        "                fn += torch.sum(1-torch.max(outputs, 1)[1] & label)\n",
        "                \n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "           \n",
        "            sentiment_acc = float(sentiment_corrects) / dataset_sizes[phase]\n",
        "\n",
        "            if phase == 'train':\n",
        "                train_acc.append(sentiment_acc)\n",
        "                train_loss.append(epoch_loss)\n",
        "            elif phase == 'val':\n",
        "                val_acc.append(sentiment_acc)\n",
        "                val_loss.append(epoch_loss)\n",
        "\n",
        "            print('{} total loss: {:.4f} '.format(phase,epoch_loss ))\n",
        "            print('{} sentiment_acc: {:.4f}'.format(\n",
        "                phase, sentiment_acc))\n",
        "\n",
        "            if phase == 'val' and epoch_loss < best_loss:\n",
        "                print('saving with loss of {}'.format(epoch_loss),\n",
        "                      'improved over previous {}'.format(best_loss))\n",
        "                best_loss = epoch_loss\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                name = str(type(model))\n",
        "                torch.save(model.state_dict(), '../../Models/Weights/'+ name[name.index('.')+1:-2] +'_model_test.pth')\n",
        "\n",
        "            if phase == 'val' and epoch == num_epochs - 1:\n",
        "                recall = tp / (tp + fn)\n",
        "                print('recall {:.4f}'.format(recall))\n",
        "\n",
        "        print()\n",
        "\n",
        "    confusion_matrix = [[int(tp), int(fp)],[int(fn), int(tn)]]\n",
        "    precision = tp / (tp + fp)\n",
        "    f1 = 2*(precision*recall)/(precision+recall)\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val loss: {:4f}'.format(float(best_loss)))\n",
        "\n",
        "    results = {'time': time_elapsed, \n",
        "               'recall': recall,\n",
        "               'precision': precision,\n",
        "               'f1': f1, \n",
        "               'conf_matr': confusion_matrix,\n",
        "               'val_loss': val_loss, \n",
        "               'train_loss': train_loss, \n",
        "               'val_acc': val_acc, \n",
        "               'train_acc': train_acc}\n",
        "\n",
        "    mistakes = np.array(uncorrect_texts)\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, results, mistakes, correct_labels.astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7aFQTvSDekB",
        "colab_type": "text"
      },
      "source": [
        "Train model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ov772HfUDb93",
        "colab_type": "code",
        "outputId": "56e2e666-feed-407d-b5da-2ee783af9c0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "_, _, mistakes, correct_labels = train_model(model, criterion, optimizer, scheduler = None, num_epochs=20)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "starting\n",
            "Epoch 1/20\n",
            "----------\n",
            "train total loss: 0.7518 \n",
            "train sentiment_acc: 0.8044\n",
            "val total loss: 0.7592 \n",
            "val sentiment_acc: 0.7954\n",
            "saving with loss of 0.7591504617057256 improved over previous 200\n",
            "\n",
            "Epoch 2/20\n",
            "----------\n",
            "train total loss: 0.7196 \n",
            "train sentiment_acc: 0.8253\n",
            "val total loss: 0.7259 \n",
            "val sentiment_acc: 0.8160\n",
            "saving with loss of 0.7258594654641062 improved over previous 0.7591504617057256\n",
            "\n",
            "Epoch 3/20\n",
            "----------\n",
            "train total loss: 0.6952 \n",
            "train sentiment_acc: 0.8421\n",
            "val total loss: 0.7192 \n",
            "val sentiment_acc: 0.8251\n",
            "saving with loss of 0.7192030641517799 improved over previous 0.7258594654641062\n",
            "\n",
            "Epoch 4/20\n",
            "----------\n",
            "train total loss: 0.6988 \n",
            "train sentiment_acc: 0.8412\n",
            "val total loss: 0.7088 \n",
            "val sentiment_acc: 0.8323\n",
            "saving with loss of 0.7087544359001223 improved over previous 0.7192030641517799\n",
            "\n",
            "Epoch 5/20\n",
            "----------\n",
            "train total loss: 0.6706 \n",
            "train sentiment_acc: 0.8619\n",
            "val total loss: 0.6961 \n",
            "val sentiment_acc: 0.8401\n",
            "saving with loss of 0.6960579120637985 improved over previous 0.7087544359001223\n",
            "\n",
            "Epoch 6/20\n",
            "----------\n",
            "train total loss: 0.6966 \n",
            "train sentiment_acc: 0.8438\n",
            "val total loss: 0.6999 \n",
            "val sentiment_acc: 0.8381\n",
            "\n",
            "Epoch 7/20\n",
            "----------\n",
            "train total loss: 0.6549 \n",
            "train sentiment_acc: 0.8708\n",
            "val total loss: 0.6831 \n",
            "val sentiment_acc: 0.8477\n",
            "saving with loss of 0.6830988873725667 improved over previous 0.6960579120637985\n",
            "\n",
            "Epoch 8/20\n",
            "----------\n",
            "train total loss: 0.6460 \n",
            "train sentiment_acc: 0.8777\n",
            "val total loss: 0.6881 \n",
            "val sentiment_acc: 0.8456\n",
            "\n",
            "Epoch 9/20\n",
            "----------\n",
            "train total loss: 0.6487 \n",
            "train sentiment_acc: 0.8769\n",
            "val total loss: 0.6836 \n",
            "val sentiment_acc: 0.8501\n",
            "\n",
            "Epoch 10/20\n",
            "----------\n",
            "train total loss: 0.6358 \n",
            "train sentiment_acc: 0.8847\n",
            "val total loss: 0.6829 \n",
            "val sentiment_acc: 0.8487\n",
            "saving with loss of 0.6829187256485161 improved over previous 0.6830988873725667\n",
            "\n",
            "Epoch 11/20\n",
            "----------\n",
            "train total loss: 0.6237 \n",
            "train sentiment_acc: 0.8941\n",
            "val total loss: 0.6720 \n",
            "val sentiment_acc: 0.8583\n",
            "saving with loss of 0.671972299896696 improved over previous 0.6829187256485161\n",
            "\n",
            "Epoch 12/20\n",
            "----------\n",
            "train total loss: 0.6270 \n",
            "train sentiment_acc: 0.8920\n",
            "val total loss: 0.6719 \n",
            "val sentiment_acc: 0.8583\n",
            "saving with loss of 0.6719160354862174 improved over previous 0.671972299896696\n",
            "\n",
            "Epoch 13/20\n",
            "----------\n",
            "train total loss: 0.6156 \n",
            "train sentiment_acc: 0.8995\n",
            "val total loss: 0.6605 \n",
            "val sentiment_acc: 0.8658\n",
            "saving with loss of 0.6605312546844002 improved over previous 0.6719160354862174\n",
            "\n",
            "Epoch 14/20\n",
            "----------\n",
            "train total loss: 0.6060 \n",
            "train sentiment_acc: 0.9065\n",
            "val total loss: 0.6594 \n",
            "val sentiment_acc: 0.8674\n",
            "saving with loss of 0.6594346411822977 improved over previous 0.6605312546844002\n",
            "\n",
            "Epoch 15/20\n",
            "----------\n",
            "train total loss: 0.6029 \n",
            "train sentiment_acc: 0.9092\n",
            "val total loss: 0.6688 \n",
            "val sentiment_acc: 0.8629\n",
            "\n",
            "Epoch 16/20\n",
            "----------\n",
            "train total loss: 0.6140 \n",
            "train sentiment_acc: 0.9013\n",
            "val total loss: 0.6768 \n",
            "val sentiment_acc: 0.8531\n",
            "\n",
            "Epoch 17/20\n",
            "----------\n",
            "train total loss: 0.5939 \n",
            "train sentiment_acc: 0.9149\n",
            "val total loss: 0.6871 \n",
            "val sentiment_acc: 0.8464\n",
            "\n",
            "Epoch 18/20\n",
            "----------\n",
            "train total loss: 0.5921 \n",
            "train sentiment_acc: 0.9160\n",
            "val total loss: 0.6575 \n",
            "val sentiment_acc: 0.8690\n",
            "saving with loss of 0.6575369979850401 improved over previous 0.6594346411822977\n",
            "\n",
            "Epoch 19/20\n",
            "----------\n",
            "train total loss: 0.5871 \n",
            "train sentiment_acc: 0.9193\n",
            "val total loss: 0.6529 \n",
            "val sentiment_acc: 0.8718\n",
            "saving with loss of 0.6528747848994577 improved over previous 0.6575369979850401\n",
            "\n",
            "Epoch 20/20\n",
            "----------\n",
            "train total loss: 0.5799 \n",
            "train sentiment_acc: 0.9243\n",
            "val total loss: 0.6650 \n",
            "val sentiment_acc: 0.8636\n",
            "recall 0.8194\n",
            "\n",
            "Training complete in 2m 1s\n",
            "Best val loss: 0.652875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eafYAopMnCB4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mistakes_list = []\n",
        "for batch in mistakes:\n",
        "  for headline in batch:\n",
        "    mistakes_list.append(headline)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gALHQ1KTn_w8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "detokenized_text = [] \n",
        "for i in range(len(mistakes_list)):\n",
        "    headline = ' '.join(mistakes_list[i]) \n",
        "    headline = re.sub(' <pad>|<unk>', '', headline)\n",
        "    detokenized_text.append(headline) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nKnKAB00yKh",
        "colab_type": "text"
      },
      "source": [
        "Some examples, where model made mistake:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73XBuGtZoxQi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        },
        "outputId": "b361b555-ed29-4210-e079-f0fa518288f3"
      },
      "source": [
        "df = pd.DataFrame({'Mistakes of the model': detokenized_text, 'Real Label': correct_labels.tolist()})\n",
        "df.head(20)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Mistakes of the model</th>\n",
              "      <th>Real Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>hamilton</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>receiving thanks</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>permission denied</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>naked leadership</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>april cruelty</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>quantum lip</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>doing  time</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>robin hood foundation</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>refugee blues</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>family business</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>migration</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>advice</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>approved</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>snow angel</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>moore</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>men  badly</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>bolivia joins</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>to hope again</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>mutual selection process</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>more cats made</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Mistakes of the model  Real Label\n",
              "0                  hamilton            0\n",
              "1           receiving thanks           0\n",
              "2          permission denied           0\n",
              "3           naked leadership           0\n",
              "4              april cruelty           0\n",
              "5                quantum lip           0\n",
              "6                doing  time           0\n",
              "7      robin hood foundation           0\n",
              "8              refugee blues           0\n",
              "9            family business           0\n",
              "10                 migration           0\n",
              "11                    advice           0\n",
              "12                 approved            0\n",
              "13                snow angel           0\n",
              "14                     moore           0\n",
              "15                men  badly           0\n",
              "16            bolivia joins            1\n",
              "17             to hope again           0\n",
              "18  mutual selection process           0\n",
              "19            more cats made           1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    }
  ]
}