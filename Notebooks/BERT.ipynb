{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "BERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbGbIZA5E0u8",
        "colab_type": "code",
        "outputId": "a75ec943-eb9d-4aab-b8c3-59caf22a3a4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1J4V7cYeEs2V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "afa20061-d380-4ff6-ec33-91031c9963c4"
      },
      "source": [
        "# !cp -avr /content/drive/'My Drive'/DetectingSarcasm ."
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'/content/drive/My Drive/DetectingSarcasm' -> './DetectingSarcasm'\n",
            "'/content/drive/My Drive/DetectingSarcasm/Models' -> './DetectingSarcasm/Models'\n",
            "'/content/drive/My Drive/DetectingSarcasm/Models/LSTM2DMaxPool.py' -> './DetectingSarcasm/Models/LSTM2DMaxPool.py'\n",
            "'/content/drive/My Drive/DetectingSarcasm/Models/AttantionLSTM.py' -> './DetectingSarcasm/Models/AttantionLSTM.py'\n",
            "'/content/drive/My Drive/DetectingSarcasm/Models/BidirectionalLSTM.py' -> './DetectingSarcasm/Models/BidirectionalLSTM.py'\n",
            "'/content/drive/My Drive/DetectingSarcasm/Models/LSTM.py' -> './DetectingSarcasm/Models/LSTM.py'\n",
            "'/content/drive/My Drive/DetectingSarcasm/Notebooks' -> './DetectingSarcasm/Notebooks'\n",
            "'/content/drive/My Drive/DetectingSarcasm/Notebooks/TuneLSTMs.ipynb' -> './DetectingSarcasm/Notebooks/TuneLSTMs.ipynb'\n",
            "'/content/drive/My Drive/DetectingSarcasm/Notebooks/CleanedDataLSTMs.ipynb' -> './DetectingSarcasm/Notebooks/CleanedDataLSTMs.ipynb'\n",
            "'/content/drive/My Drive/DetectingSarcasm/Notebooks/LSTMs.ipynb' -> './DetectingSarcasm/Notebooks/LSTMs.ipynb'\n",
            "'/content/drive/My Drive/DetectingSarcasm/Notebooks/BERT.ipynb' -> './DetectingSarcasm/Notebooks/BERT.ipynb'\n",
            "'/content/drive/My Drive/DetectingSarcasm/Notebooks/Weights' -> './DetectingSarcasm/Notebooks/Weights'\n",
            "'/content/drive/My Drive/DetectingSarcasm/Data' -> './DetectingSarcasm/Data'\n",
            "'/content/drive/My Drive/DetectingSarcasm/Data/Sarcasm_Headlines_Dataset_v2.json' -> './DetectingSarcasm/Data/Sarcasm_Headlines_Dataset_v2.json'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wQF1RF7cnOF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a392cb3b-0fec-451f-cec2-46116cebe9eb"
      },
      "source": [
        "# %cd DetectingSarcasm/Notebooks"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/DetectingSarcasm/Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fP0gcU3pHDEc",
        "colab_type": "code",
        "outputId": "35bddab1-4fe1-4797-9d0e-0fe4ea5822f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        }
      },
      "source": [
        "!pip install pytorch-pretrained-bert"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-pretrained-bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 25.5MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 2.4MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 1.9MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 3.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 2.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.17.5)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.3.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.10.47)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.28.1)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.47 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.13.47)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.8)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->pytorch-pretrained-bert) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->pytorch-pretrained-bert) (2.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.14.0,>=1.13.47->boto3->pytorch-pretrained-bert) (1.12.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHHVoxcGEq2m",
        "colab_type": "code",
        "outputId": "f3073316-4c73-4579-c00e-ebf5f2a2e687",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "import torch\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
        "\n",
        "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:pytorch_pretrained_bert.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache, downloading to /tmp/tmpw3yigwnp\n",
            "100%|██████████| 231508/231508 [00:00<00:00, 6149127.87B/s]\n",
            "INFO:pytorch_pretrained_bert.file_utils:copying /tmp/tmpw3yigwnp to cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "INFO:pytorch_pretrained_bert.file_utils:creating metadata file for /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "INFO:pytorch_pretrained_bert.file_utils:removing temp file /tmp/tmpw3yigwnp\n",
            "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmNBPdvlEq2t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from random import randrange\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from pytorch_pretrained_bert import BertConfig\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_lGSB6iEq3W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertLayerNorm(nn.Module):\n",
        "        def __init__(self, hidden_size, eps=1e-12):\n",
        "            \"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n",
        "            \"\"\"\n",
        "            super(BertLayerNorm, self).__init__()\n",
        "            self.weight = nn.Parameter(torch.ones(hidden_size))\n",
        "            self.bias = nn.Parameter(torch.zeros(hidden_size))\n",
        "            self.variance_epsilon = eps\n",
        "\n",
        "        def forward(self, x):\n",
        "            u = x.mean(-1, keepdim=True)\n",
        "            s = (x - u).pow(2).mean(-1, keepdim=True)\n",
        "            x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
        "            return self.weight * x + self.bias\n",
        "        \n",
        "\n",
        "class BertForSequenceClassification(nn.Module):\n",
        "    \"\"\"BERT model for classification.\n",
        "    This module is composed of the BERT model with a linear layer on top of\n",
        "    the pooled output.\n",
        "    Params:\n",
        "        `config`: a BertConfig class instance with the configuration to build a new model.\n",
        "        `num_labels`: the number of classes for the classifier. Default = 2.\n",
        "    Inputs:\n",
        "        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
        "            with the word token indices in the vocabulary. Items in the batch should begin with the special \"CLS\" token. (see the tokens preprocessing logic in the scripts\n",
        "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
        "        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
        "            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
        "            a `sentence B` token (see BERT paper for more details).\n",
        "        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
        "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
        "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
        "            a batch has varying length sentences.\n",
        "        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size]\n",
        "            with indices selected in [0, ..., num_labels].\n",
        "    Outputs:\n",
        "        if `labels` is not `None`:\n",
        "            Outputs the CrossEntropy classification loss of the output with the labels.\n",
        "        if `labels` is `None`:\n",
        "            Outputs the classification logits of shape [batch_size, num_labels].\n",
        "    Example usage:\n",
        "    ```python\n",
        "    # Already been converted into WordPiece token ids\n",
        "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
        "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
        "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
        "    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
        "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
        "    num_labels = 2\n",
        "    model = BertForSequenceClassification(config, num_labels)\n",
        "    logits = model(input_ids, token_type_ids, input_mask)\n",
        "    ```\n",
        "    \"\"\"\n",
        "    def __init__(self, config, num_labels=2):\n",
        "        super(BertForSequenceClassification, self).__init__()\n",
        "        self.num_labels = num_labels\n",
        "        self.config = config\n",
        "        #self.bert = BertModel(config=config) # Uncomment if you want to do castom config\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
        "        nn.init.xavier_normal_(self.classifier.weight)\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
        "        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        return logits\n",
        "    def freeze_bert_encoder(self):\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = False\n",
        "    \n",
        "    def unfreeze_bert_encoder(self):\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oa-lw9IVEq3a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
        "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
        "\n",
        "num_labels = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xX6evit25eVu",
        "colab_type": "text"
      },
      "source": [
        "Defining BERT model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNG4bwXXgtor",
        "colab_type": "code",
        "outputId": "de428d45-4e91-4a0d-e01b-644cb99bb601",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        }
      },
      "source": [
        "model = BertForSequenceClassification(config, num_labels)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:pytorch_pretrained_bert.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz not found in cache, downloading to /tmp/tmpigjrjvv1\n",
            "100%|██████████| 407873900/407873900 [00:05<00:00, 77512063.62B/s]\n",
            "INFO:pytorch_pretrained_bert.file_utils:copying /tmp/tmpigjrjvv1 to cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
            "INFO:pytorch_pretrained_bert.file_utils:creating metadata file for /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
            "INFO:pytorch_pretrained_bert.file_utils:removing temp file /tmp/tmpigjrjvv1\n",
            "INFO:pytorch_pretrained_bert.modeling:loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
            "INFO:pytorch_pretrained_bert.modeling:extracting archive file /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpv7vw5y6a\n",
            "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVs2vsda5rWp",
        "colab_type": "text"
      },
      "source": [
        "Prepearing data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_u2XuqREEq3g",
        "colab_type": "code",
        "outputId": "166ae601-2a10-4235-ee44-94c6703990d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_json(\"../Data/Sarcasm_Headlines_Dataset_v2.json\", lines=True)\n",
        "df.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>is_sarcastic</th>\n",
              "      <th>headline</th>\n",
              "      <th>article_link</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>thirtysomething scientists unveil doomsday clo...</td>\n",
              "      <td>https://www.theonion.com/thirtysomething-scien...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>dem rep. totally nails why congress is falling...</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/donna-edw...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>eat your veggies: 9 deliciously different recipes</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/eat-your-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>inclement weather prevents liar from getting t...</td>\n",
              "      <td>https://local.theonion.com/inclement-weather-p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>mother comes pretty close to using word 'strea...</td>\n",
              "      <td>https://www.theonion.com/mother-comes-pretty-c...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   is_sarcastic  ...                                       article_link\n",
              "0             1  ...  https://www.theonion.com/thirtysomething-scien...\n",
              "1             0  ...  https://www.huffingtonpost.com/entry/donna-edw...\n",
              "2             0  ...  https://www.huffingtonpost.com/entry/eat-your-...\n",
              "3             1  ...  https://local.theonion.com/inclement-weather-p...\n",
              "4             1  ...  https://www.theonion.com/mother-comes-pretty-c...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEPcBlxzc2En",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "outputId": "8085b357-7dd4-4c09-8e68-bec3bb9f8c69"
      },
      "source": [
        "x = df['is_sarcastic'].value_counts()\n",
        "plt.pie(x, labels = ['non sarcastic', 'sarcastic'], shadow=True)\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94YCTwJLH1RS",
        "colab_type": "code",
        "outputId": "69cf40ee-42e2-47ed-8262-fe65e8eacd0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "df = df.drop(['article_link'], axis=1)\n",
        "df.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>is_sarcastic</th>\n",
              "      <th>headline</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>thirtysomething scientists unveil doomsday clo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>dem rep. totally nails why congress is falling...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>eat your veggies: 9 deliciously different recipes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>inclement weather prevents liar from getting t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>mother comes pretty close to using word 'strea...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   is_sarcastic                                           headline\n",
              "0             1  thirtysomething scientists unveil doomsday clo...\n",
              "1             0  dem rep. totally nails why congress is falling...\n",
              "2             0  eat your veggies: 9 deliciously different recipes\n",
              "3             1  inclement weather prevents liar from getting t...\n",
              "4             1  mother comes pretty close to using word 'strea..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvSxEUK2Eq4H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = df['headline']\n",
        "y = df['is_sarcastic']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOX0oLliEq4O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.tolist()\n",
        "X_test = X_test.tolist()\n",
        "\n",
        "y_train = y_train.tolist()\n",
        "y_test = y_test.tolist()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PypbNzeS5282",
        "colab_type": "text"
      },
      "source": [
        "As most headlines have length less then 24, we will define max_seq_length = 24:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBCLwuUjEq4T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_seq_length = 24\n",
        "class text_dataset(Dataset):\n",
        "    def __init__(self,x_y_list, transform=None):\n",
        "        \n",
        "        self.x_y_list = x_y_list\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __getitem__(self,index):\n",
        "        \n",
        "        tokenized_review = tokenizer.tokenize(self.x_y_list[0][index])\n",
        "        \n",
        "        if len(tokenized_review) > max_seq_length:\n",
        "            tokenized_review = tokenized_review[:max_seq_length]\n",
        "            \n",
        "        ids_review  = tokenizer.convert_tokens_to_ids(tokenized_review)\n",
        "\n",
        "        padding = [0] * (max_seq_length - len(ids_review))\n",
        "        \n",
        "        ids_review += padding\n",
        "        \n",
        "        assert len(ids_review) == max_seq_length\n",
        "        \n",
        "        #print(ids_review)\n",
        "        ids_review = torch.tensor(ids_review)\n",
        "        \n",
        "        sentiment = self.x_y_list[1][index] # color        \n",
        "        list_of_labels = [torch.from_numpy(np.array(sentiment))]\n",
        "        \n",
        "        \n",
        "        return ids_review, list_of_labels[0]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.x_y_list[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3-3W5cSEq4V",
        "colab_type": "code",
        "outputId": "3b809bd1-c0b5-4e01-a333-468571e83ee5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_lists = [X_train, y_train]\n",
        "test_lists = [X_test, y_test]\n",
        "\n",
        "training_dataset = text_dataset(x_y_list = train_lists )\n",
        "\n",
        "test_dataset = text_dataset(x_y_list = test_lists )\n",
        "\n",
        "dataloaders_dict = {'train': torch.utils.data.DataLoader(training_dataset, batch_size=batch_size, shuffle=True, num_workers=0),\n",
        "                   'val':torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "                   }\n",
        "dataset_sizes = {'train':len(train_lists[0]),\n",
        "                'val':len(test_lists[0])}\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vteX76PFEq4Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "    print('starting')\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss = 100\n",
        "\n",
        "    val_loss = []\n",
        "    train_loss = []\n",
        "    val_acc = []\n",
        "    train_acc = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0          \n",
        "            sentiment_corrects = 0\n",
        "            tp = 0.0 #true poisitve\n",
        "            tn = 0.0 #true negative\n",
        "            fp = 0.0 #false positive\n",
        "            fn = 0.0 #false negative\n",
        "            \n",
        "            # Iterate over data.\n",
        "            for inputs, sentiment in dataloaders_dict[phase]:\n",
        "\n",
        "                inputs = inputs.to(device) \n",
        "                sentiment = sentiment.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "\n",
        "                    outputs = model(inputs)\n",
        "                    outputs = F.softmax(outputs,dim=1)\n",
        "\n",
        "                    loss = criterion(outputs, sentiment)\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        \n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "                sentiment_corrects += torch.sum(torch.max(outputs, 1)[1] == sentiment)\n",
        "\n",
        "                tp += torch.sum(torch.max(outputs, 1)[1] & sentiment)\n",
        "                tn += torch.sum(1-torch.max(outputs, 1)[1] & 1-sentiment)\n",
        "                fp += torch.sum(torch.max(outputs, 1)[1] & 1-sentiment)\n",
        "                fn += torch.sum(1-torch.max(outputs, 1)[1] & sentiment)\n",
        "\n",
        "                \n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "\n",
        "            sentiment_acc = sentiment_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            if phase == 'train':\n",
        "                train_acc.append(sentiment_acc)\n",
        "                train_loss.append(epoch_loss)\n",
        "            elif phase == 'val':\n",
        "                val_acc.append(sentiment_acc)\n",
        "                val_loss.append(epoch_loss)\n",
        "\n",
        "            print('{} total loss: {:.4f} '.format(phase,epoch_loss ))\n",
        "            print('{} sentiment_acc: {:.4f}'.format(\n",
        "                phase, sentiment_acc))\n",
        "\n",
        "            if phase == 'val' and epoch_loss < best_loss:\n",
        "                print('saving with loss of {}'.format(epoch_loss),\n",
        "                      'improved over previous {}'.format(best_loss))\n",
        "                best_loss = epoch_loss\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                torch.save(model.state_dict(), '../Models/Weights/bert_model_test.pth')\n",
        "\n",
        "            if phase == 'val' and epoch == num_epochs - 1:\n",
        "                recall = tp / (tp + fn)\n",
        "                print('recall {:.4f}'.format(recall))\n",
        "\n",
        "        print()\n",
        "    \n",
        "    confusion_matrix = [[int(tp), int(fp)],[int(fn), int(tn)]]\n",
        "    precision = tp / (tp + fp)\n",
        "    f1 = 2*(precision*recall)/(precision+recall)\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val loss: {:4f}'.format(float(best_loss)))\n",
        "\n",
        "    results = {'time': time_elapsed, \n",
        "               'recall': recall,\n",
        "               'precision': precision,\n",
        "               'f1': f1, \n",
        "               'conf_matr': confusion_matrix,\n",
        "               'val_loss': val_loss, \n",
        "               'train_loss': train_loss, \n",
        "               'val_acc': val_acc, \n",
        "               'train_acc': train_acc} \n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Bpdj3zFEq4b",
        "colab_type": "code",
        "outputId": "d24e313e-4154-4bad-ab24-00733c65c573",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.to(device)\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BQYohJw7V8Y",
        "colab_type": "text"
      },
      "source": [
        "We will use two learning rates: 1st is for classifier, so it can be more aggressive and 2nd one for pre-trained bert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpirB1t1Eq4d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lrlast = .001\n",
        "lrmain = .00001\n",
        "optimizer = optim.Adam(\n",
        "    [\n",
        "        {\"params\":model.bert.parameters(),\"lr\": lrmain},\n",
        "        {\"params\":model.classifier.parameters(), \"lr\": lrlast},\n",
        "       \n",
        "   ])\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 4 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "rDOkYH2_Eq4h",
        "colab_type": "code",
        "outputId": "053063c1-ab8d-4fb1-9999-3a90b0bc809f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model_fit, res = train_model(model, criterion, optimizer, exp_lr_scheduler,\n",
        "                       num_epochs=10)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "starting\n",
            "Epoch 1/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train total loss: 0.5072 \n",
            "train sentiment_acc: 0.7905\n",
            "val total loss: 0.4482 \n",
            "val sentiment_acc: 0.8595\n",
            "saving with loss of 0.44819194078445435 improved over previous 100\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n",
            "train total loss: 0.4292 \n",
            "train sentiment_acc: 0.8785\n",
            "val total loss: 0.5010 \n",
            "val sentiment_acc: 0.8066\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n",
            "train total loss: 0.4095 \n",
            "train sentiment_acc: 0.9009\n",
            "val total loss: 0.4260 \n",
            "val sentiment_acc: 0.8852\n",
            "saving with loss of 0.42604762305337013 improved over previous 0.44819194078445435\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n",
            "train total loss: 0.3845 \n",
            "train sentiment_acc: 0.9261\n",
            "val total loss: 0.4124 \n",
            "val sentiment_acc: 0.8969\n",
            "saving with loss of 0.41240862251411026 improved over previous 0.42604762305337013\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n",
            "train total loss: 0.3800 \n",
            "train sentiment_acc: 0.9313\n",
            "val total loss: 0.4127 \n",
            "val sentiment_acc: 0.8957\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n",
            "train total loss: 0.3755 \n",
            "train sentiment_acc: 0.9362\n",
            "val total loss: 0.4085 \n",
            "val sentiment_acc: 0.9013\n",
            "saving with loss of 0.4085282393679762 improved over previous 0.41240862251411026\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n",
            "train total loss: 0.3736 \n",
            "train sentiment_acc: 0.9376\n",
            "val total loss: 0.4054 \n",
            "val sentiment_acc: 0.9041\n",
            "saving with loss of 0.40541954131329666 improved over previous 0.4085282393679762\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n",
            "train total loss: 0.3691 \n",
            "train sentiment_acc: 0.9424\n",
            "val total loss: 0.4089 \n",
            "val sentiment_acc: 0.9009\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n",
            "train total loss: 0.3692 \n",
            "train sentiment_acc: 0.9431\n",
            "val total loss: 0.4087 \n",
            "val sentiment_acc: 0.9016\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n",
            "train total loss: 0.3694 \n",
            "train sentiment_acc: 0.9420\n",
            "val total loss: 0.4093 \n",
            "val sentiment_acc: 0.8994\n",
            "recall 0.8739\n",
            "\n",
            "Training complete in 15m 18s\n",
            "Best val loss: 0.405420\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezNKXtnk87kA",
        "colab_type": "text"
      },
      "source": [
        "Lets see at test and train accuracy plots:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8THlZmAt8uab",
        "colab_type": "code",
        "outputId": "a81b1dee-b4b4-4e18-8076-04dc3dd09e69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        }
      },
      "source": [
        "plt.plot(res['train_acc'], label = 'Train Accuracy')\n",
        "plt.plot(res['val_acc'], label = 'Test Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEGCAYAAABrQF4qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3deXxU1f3/8dcn+x4IhC0BgghC2CGC\nuAticat7lUoVXHBD61Kt/dVu1lq7WLug7ZcquBa0WlG/1fJVwKUqS9jJArKGJEBCAgkQsk0+vz/u\nJExCgIFMZpLJ5/l45DF37r1z75lR3nPm3HPPEVXFGGNM8AoJdAGMMca0Lgt6Y4wJchb0xhgT5Czo\njTEmyFnQG2NMkAsLdAGa6tq1q6alpQW6GMYY066sXLlyr6omN7etzQV9WloamZmZgS6GMca0KyKy\n41jbrOnGGGOCnAW9McYEOQt6Y4wJchb0xhgT5CzojTEmyFnQG2NMkLOgN8aYINfm+tEbY4w3XHVK\njauOqto6qmvrqHa5H5s+91zvcrkftdG+ihIdHkpMRChR4aHERIQRExFKdERow/ojy2FEhYcgIoH+\nCLxmQW+M8auSg1Xk7DpA9q4ytu09RGXNcUK6tu5ImLsab3fVBW4uDRGIDneCPzrC/UXQsBx21BdE\nTHgY0REhREeEEePez1lfv7+zLS4yjMTocJ+X16ugF5HJwJ+AUOBFVX2myfa+wBwgGSgFpqpqvsf2\nBCAbWKCqM31UdmNMG+aqU7aXHCJnVznZheXO465y9pRXNeyTFBtBTEQoEWEhRISGEBkWQkRYCFHh\nIcRHhRER6jyPCHNv83geXr/s8Trneah7u3i8LvTI9vrjeBxLgMM1Luev2kVFtYuK6tpGzxstu7c5\ny65Gy0UHKp111S4q3K+pqq3z6jMbkZrIezPP9fl/ixMGvYiEAs8Dk4B8YIWIvK+q2R67/R54VVVf\nEZEJwK+B73ls/yXwue+KbYxpSw5V1ZK7+wDZu8obgn3j7gMcrnEBEBYinN4tjrP7dyW9ZwLpvRIY\n3DOBpNiIAJf8iNjIMGIjW6eRw1WnDV8UzhdALRXVLirrv1RqnOWE6NY5vzdHHQtsVtWtACIyH7gK\np4ZeLx142L28BFhQv0FExgDdgf8AGT4oszEmQFSV3eWVHrV0J9y3lxyiflbShKgwBvdM4KaxvRnc\nM4H0ngkM6B5HZFhoYAsfQKEhQlyk0zQTCN6cNQXY6fE8HxjXZJ+1wLU4zTvXAPEi0gXYBzwLTAUu\nPtYJRGQGMAOgT58+3pbdGNOKalx1bC462BDq9bX1fRU1Dfv0SYohvWcCV49McdfS40npFN2uLlR2\nBL76evkBMEtEpuE00RQALuBe4ENVzT/ef3hVnQ3MBsjIyLDZyo3xs7KKGrJ3lTdqetlcdJBql9O2\nHBkWwhk94vnWkB5OLb1XAoN6xBMf5fsLh8b3vAn6AqC3x/NU97oGqlqIU6NHROKA61R1v4iMB84T\nkXuBOCBCRA6q6uM+Kb0x5rhcdcrBqloOuf8OuB/LDtewafcBsncdIGdXOQX7Dze8pmtcJOm9Ejhv\noLs9vWcC/brGEhZqt920V94E/QpggIj0wwn4m4Dveu4gIl2BUlWtA36E0wMHVb3ZY59pQIaFvDHH\nV11b1xDOB5sE9KGqWg5U1nKoysWhamf7wUqPfaud5werXByqqm24GNqcEIHTkuMY07czU8/q29D0\n0i0+yo/v1vjDCYNeVWtFZCawEKd75RxVzRKRJ4FMVX0fuBD4tYgoTtPNfa1YZmPaFVedsm3voYZm\nkdKD1U5Ae4T5kVq3q6G55ESiwkMaLvDV9xjpHh/FaV2d5fioMGIjwoiNDHX2i3LWx7m3pXWJJSq8\n414g7UhEtW01iWdkZKjNMGXaq8oaF5v2HCC7sJyswnKyCsvI3X2AimqnZh0eKiTFRjQEbn1IN14O\nPXpdVON9YyNCrSnFNCIiK1W12Z6NdmesMaeo7HANObuOBHr9Bcxa9x2bcZFhpPdM4DsZvRnSy7mA\nOaBbPBFhFtDGvyzojTkBVWVPeRXZu8rIKnAH+64ydpYeuYCZHB/JkF4JTBzcjSG9EknvmUCfpBhC\nQqyboQk8C3pjPNS5b9vPKmxcUy85VN2wT1qXGIandOKmM/uQ3iuBIb0S7AKmadMs6E2HVVXr4ps9\nB8kqLGsI9pxd5Y3a0wd0i2fCoG7uppdEBve0vuOm/bGgNx3CgcoajwukTk3dsz09NiKU9F5Oe3p6\nL6fv+MDu1p5ugoMFvQk65ZU1bCgoY0NBGesLytlQ4AyHW69rnNOe7tTUE0nvlUBfa083QcyC3rRr\nZYdryCooY737b0NBGdtLKhq290qMYmhKIteOSmFoSqLTnp5g7emmY7GgN+1GWUUNGwobh/oOj1BP\n6RTN0JQErh+TytCURIalJNIlLjKAJTambbCgN23S/opqNhSUNwT6+oIy8kobh/qwlES+k9G7IdTb\n0tjmxrQlFvQm4PYdqm5US19fUEb+viN91HsnOaF+09jeDEtJZGivRDpbqBvjNQt641el7lDfUFDG\n+nwn1D1HTuyTFMOI1E7cPK6vE+opCXSKsVA3piUs6E2r2llawXtrCtzh3ng43L5dYhjZpxPfG9+3\noaaeGGN91I3xNQt602qyCsv43kvLKT1UTVqXGEb16cQt7lAfkpLYKrPdG2OOZkFvWsWanfu55aVl\nxEWGseiRC+ifHBfoIhnTYVnQG59bsb2U6XNXkBQbwRt3jKN3Ukygi2RMh2ZBb3zqy817ueOVTHp2\niuIfd5xFj0S7OcmYQLOBPIzPLMktYvrLK+jbJYY3Z4y3kDemjfAq6EVksohsFJHNInLUnK8i0ldE\nFonIOhH5VERS3etHisjXIpLl3najr9+AaRv+s2E3M17LZGD3OObdeRbJ8XZHqjFtxQmDXkRCgeeB\nS4F0YIqIpDfZ7ffAq6o6HHgS+LV7fQVwi6oOASYDfxSRTr4qvGkb3ltTwH3/WMXQlETeuOMsu5nJ\nmDbGmxr9WGCzqm5V1WpgPnBVk33SgcXu5SX121V1k6p+414uBIqAZF8U3LQNb2Xu5ME315DRtzOv\n3T7Oukwa0wZ5E/QpwE6P5/nudZ7WAte6l68B4kWki+cOIjIWiAC2ND2BiMwQkUwRySwuLva27CbA\nXvt6O4+9vY5zT+/Ky9PHEhdp1/aNaYt8dTH2B8AFIrIauAAoAFz1G0WkJ/AaMF1V65q+WFVnq2qG\nqmYkJ1uFvz148Yut/OS9LC4e3I2/35JBdERooItkjDkGb6pgBUBvj+ep7nUN3M0y1wKISBxwnaru\ndz9PAP4N/FhVl/qi0Caw/rLoG579eBOXD+vJczeOtFmYjGnjvPkXugIYICL9RCQCuAl433MHEekq\nIvXH+hEwx70+AngX50Lt274rtgkEVeV3C3N59uNNXDsqhT/dZCFvTHtwwn+lqloLzAQWAjnAW6qa\nJSJPisi33btdCGwUkU1Ad+BX7vXfAc4HponIGvffSF+/CdP6VJWn/p3D80u2MGVsb35/wwjCQi3k\njWkPRFUDXYZGMjIyNDMzM9DFMB7q6pSfvLeBN5blMe3sNH52ZToiNr+qMW2JiKxU1Yzmtlk3CXNc\nrjrlh++s4+2V+dx9QX9+OPkMC3lj2hkLenNMNa46Hn5rLR+sLeTBiwfw/YkDLOSNaYcs6E2zqmpd\nPDBvNQuz9vD4pYO4+4L+gS6SMeYUWdCbo1TWuLj79ZV8urGYn1+ZzrRz+gW6SMaYFrCgN41UVNdy\nxyuZfL21hF9fO4wpY/sEukjGmBayoDcNDlTWMH3uClbl7eMP3xnBNaNSA10kY4wPWNAbAPZXVHPr\nnOVkFZYz67ujuWxYz0AXyRjjIxb0hpKDVUx9aTlbig7yt6ljuDi9e6CLZIzxIQv6Dq6ovJLvvriM\n/H0VvDQtg/MG2KByxgQbC/oOrGD/YW7++1KKD1Tx8vSxnHValxO/yBjT7ljQd1A7Sg7x3b8vo7yy\nhldvH8eYvp0DXSRjTCuxoO+ANhcd5OYXl1JdW8e8O89iaEpioItkjGlFFvQdTO7ucqa+uAwQ5s8Y\nzxk94gNdJGNMK7NxZjuQDQVl3DR7KWEhIbx511kW8sZ0EFaj7yBW7tjHtLnLSYwO5x93nEWfLjGB\nLpJpLapQUQoHCqF8F5QXwAH3Y/kuZ/lgEcR2hcTekJgKnXpDYh/3Y2+I7wEhNj1ksLCg7wCWbi3h\ntpdX0C0+kn/ceRa9OkUHukjmVLlq4eDuYwd4/bKrqskLBeK6Q0JPSDoNUs+EihIo2wkFK+FwaePd\nQ8IgoVfj8K9/rP9yCI/y29s2LWNBH+Q+31TMjNcy6d05hjfuGEe3BPvH2WZVH2oS4IXOn2eAH9wD\nNJksKDTSCfCEFEjJgMHu5Xj3Y0JPJ+RDw4997qqDUJbvBP/+PI/lnbDtc6cMWtf4NbHdmv8SqF+O\n7uTzj8icGq+CXkQmA38CQoEXVfWZJtv74swTmwyUAlNVNd+97VbgCfeuT6nqKz4quzmBT7L3cO8b\nq+jfLY7Xbx9Ll7jIQBepY6sohV1rYN8Od4C7g7x8l7NcWXb0a6ISIb6XU7vuPsQjwN3r4ntBTBK0\ndJ6AyDjoNsj5a46rxvmyKct3wr/hC2En7F4PGz86+ldEZIJH8Kd6LLt/JcR2g5BmLhOqgqsaaqs8\nHqugttp5dNUcva622tn3qHX1x2i6zX2c+nUiEJvs8dcV4rodWY5Nhoi4ln/OAXLCqQRFJBTYBEwC\n8nEmC5+iqtke+/wT+F9VfUVEJgDTVfV7IpIEZAIZONWQlcAYVd13rPPZVIK+8d6aAh55ay1DUhJ5\ndfpYEmOOU5szvldbDXvWQ/5KKMiE/Ewo3eKxQ31TSn1ge4R3fYAn9ISI2IC9hZOiCoeK3V8CeUe+\nDBq+GPKO/iILjXDCXl2NQ72uxrdlC42EsEjnfE0f65frXFCx13kPzX3hAoRFNw7+uOQmXw4efzFd\nINS/DSYtnUpwLLBZVbe6DzYfuArI9tgnHXjYvbwEWOBe/hbwsaqWul/7MTAZmHeyb8J4R1X5y+LN\n/OHjTYzrl8SLt2YQH2Uh36pUYd+2xqG+e50TXOAEekoGjLoZUsZAl9NP3JTS3og4NeC4bpA6pvl9\nKss9wt/9a+BgkXM9oLkQDot0Qjo0/Oh1YREejxFN1nnsGxJ28rXw2io45A79hseiI88PFjlNWbvX\nO+ua/WIS55eW5y+E2Ca/EOK6+e3XgjdBnwLs9HieD4xrss9a4Fqc5p1rgHgR6XKM16Y0PYGIzABm\nAPTpY+Ofn6qqWhc/+td6/rWqgGtHp/DMtcOJCLMetD5XUQoFq46EuufFzPAY6DkSxt3lhHvKGKfZ\nop3+5PepqASIGuI0QbVlYZGQmOL8nYgqVO4/8oVwsKiZL4i9R74UjvlrIcr5Iug9Fq5/ybfvB99d\njP0BMEtEpgGfAwWAy9sXq+psYDY4TTc+KlOHsr+imrteW8mybaU8MmkgMyecbvO7+kJttfOPtKC5\nJhiB5EEw6LIjod4t3e8/2U0AiUB0Z+ev64AT79/o10KTv4PFTrfWVuDN/5EFQG+P56nudQ1UtRCn\nRo+IxAHXqep+ESkALmzy2k9bUF7TjB0lh5g+dwX5+w7zp5tGctVIL2oi5miqULq1cW29URNMD0jN\ngFFTnVDvNcqppRrjrZP5teDL03qxzwpggIj0wwn4m4Dveu4gIl2BUlWtA36E0wMHYCHwtIjUj5h1\niXu78ZHM7aXc+apz8fqNO8dxZlpSgEvkobIMVr0Gu9Y6FxUjYp22SG+XwyJbt8njRE0wvUbBuLud\nUE/NcHq82K8k0w6dMOhVtVZEZuKEdigwR1WzRORJIFNV38eptf9aRBSn6eY+92tLReSXOF8WAE/W\nX5g1LffemgIe/ec6UjpHM3famaR1bSM9NPbnwdK/wapXofqA062utsrpJ15zyPvjSGiTL4CmXwje\nfnG4/8oKPEI906m9OyeCboNh0OVHQj15sDXBmKBxwu6V/mbdK09MVXl+yWZ+/3+bGNsvif+ZOobO\nsRGBLpbT6+Trv0D2+07Nd8i1MP4+6DXyyD51dVBT4YR+9UH346Fmnh9r+RjP1etLQk5XxpQxR0K9\n1yiItHF/TPvW0u6Vpg2prq3jR/9azzur8rlmVArPXDeMyLAAjklS54KNH8LXz0Pe1xCZ6IT7uLuc\n3iZNhYQ4N+dExgE+mrJQ9cgvhuN9WcR2dS6a+rl91JhAs6BvR8oqarjr9UyWbi3loYsH8sDEAPas\nqT4Ea/4BS19wmkAS+8C3fg2jv+f/2rGIM+5KeBTE2ixZxjRlQd9O5JVUMO3l5eSXHua5G0dwzahm\nasv+cGA3LPsfyJzj9B9OyYAbfgqDrrQ2bWPaKPuX2Q6s3FHKna+upE6V1+8Yx9h+AehZs3uD0zyz\n/p9QVwuDr4DxM6H3OOuJYkwbZ0Hfxn2wtpBH/rmWXolRzJ0+ln7+7FmjCpsXwdezYOsSp8thxnQ4\n6x5nqFtjTLtgQd9GqSovfLqF3y3cyJlpnZn9vQz/9ayprYJ1bzk1+OIc50ahiT+DMdOc8TuMMe2K\nBX0bVF1bx4/fXc8/V+Zz9che/Ob64f7pWXOoBDJfguV/d8bo6D4Urv4bDL3OGTDKGNMuWdC3MWUV\nNdz9+kq+3lrC9ycO4MGLB7R+z5q9m2Hp87BmHtQehtMnOV0kT7vQ2t+NCQIW9G1IXkkF019eTl5p\nBX/4zgiuHd2KPWtUYceX8NUs2PQfZyjY4Tc6Ad9tcOud1xjjdxb0bcTKHfuY8WomtXXKa7eP46zT\nWqk/uKsGst+Dr/7izHYU0wUueAzOvMMZH9sYE3Qs6NuA/11XyMNvOT1r5kw7k9OS43x/ksoyWPmK\n0we+PN+Z/OKK52DEFAi3ycKNCWYW9AGkqvz1sy389j8byejbmdm3ZJDk6541+3bAsvoBxg5C2nlw\n+e9hwLean6/TGBN0LOgDpMZVxxPvbuDNzJ1cNbIXv7luOFHhPuxZc7AYPnrUaaZBYGj9AGOjfHcO\nY0y7YEEfAGWHa7j3jZV8ubmEByYO4KHW6Fnz+e8g53/dA4zd3fwAY8aYDsGC3s92llYw/eUV7Cg5\nxLM3jOC6Ma0QwHV1kPM+DPwWXPKU749vjGlXLOj9aHXePu58NZMal/LqbeMY37+VetbsXObMUp9+\ndesc3xjTrljQ+8mH63fx0Jtr6J4QxdzpZ9K/NXrW1MteAKGRcMbk1juHMabd8KrbhYhMFpGNIrJZ\nRB5vZnsfEVkiIqtFZJ2IXOZeHy4ir4jIehHJEZEON1+sqvK3z7Zw7xurGJqSyLv3nt26IV9X51yA\nPf1imzXJGAN4UaMXkVDgeWASkA+sEJH3VTXbY7cngLdU9a8ikg58CKQBNwCRqjpMRGKAbBGZp6rb\nffw+2qQaVx0/WbCB+St2cuWIXvzueh/3rGlO/nKn2WaINdsYYxzeNN2MBTar6lYAEZkPXAV4Br0C\nCe7lRKDQY32siIQB0UA1UO6Dcrd55ZU13Pv6Kv67eS/3Tzidhy4eSEiIH8aNyXI32wy0ZhtjjMOb\noE8Bdno8zwfGNdnn58D/icj9QCxwsXv92zhfCruAGOAhVS1tegIRmQHMAOjTp89JFL9t2llawW0v\nr2Db3kP87vrh3JDR2z8nbmi2mQhRCSfe3xjTIfjq1sgpwMuqmgpcBrwmIiE4vwZcQC+gH/CIiBw1\nY4WqzlbVDFXNSE5O9lGRAmNX2WGueeEr9pRX8urtY/0X8gD5K+BAofW2McY04k2NvgDwTKtU9zpP\ntwOTAVT1axGJAroC3wX+o6o1QJGIfAlkAFtbWvC26v01hew9WMWHD5xHei8/16qzF0BohPW2McY0\n4k2NfgUwQET6iUgEcBPwfpN98oCJACIyGIgCit3rJ7jXxwJnAbm+KXrbtCi3iME9E/wf8vXNNv0n\nQlSif89tjGnTThj0qloLzAQWAjk4vWuyRORJEfm2e7dHgDtFZC0wD5imqorTWydORLJwvjDmquq6\n1ngjbcH+impW7tjHxEEBGO63IBPKC6y3jTHmKF7dMKWqH+J0mfRc91OP5WzgnGZedxCni2WH8Nmm\nYlx1yoTBAQj6rPpmm0v9f25jTJtm49T60OLcIrrERjAitZN/T9zQbDPBmm2MMUexoPeRWlcdn24s\n5sIzuhHqj/7yngpWOpOJWG8bY0wzLOh9ZPXO/ZQdrmFiIJptshdASLg12xhjmmVB7yOLcooICxHO\nG9DVvydWPdJsE+3nJiNjTLtgQe8ji3P3MO60JOKjwv174oKVULbTetsYY47Jgt4HdpZWsGnPQSYM\n6u7/k2e96262ucz/5zbGtAsW9D6wOLcIwP/951Uh+31rtjHGHJcFvQ8syi3itORY0rrG+vfEBaug\nLM+abYwxx2VB30KHqmpZuqUkMHfDZluzjTHmxCzoW+i/m/dS7arzf/u8KmS9B/0vsmYbY8xxWdC3\n0OKcIuKjwshI6+zfExe6m23sJiljzAlY0LdAXZ2yeGMRFwxMJjzUzx9llvsmqUHWbGOMOT4L+hbY\nUFhG8YEq/98Nq+rcDXvahRDt518Sxph2x4K+BRblFBEicMFAPwd94WrYb71tjDHesaBvgcW5RYzu\n05mk2Aj/njh7AYSEWW8bY4xXLOhP0Z7yStYXlPl/7HlVp33+tAshJsm/5zbGtEsW9KdoScPdsH7u\nVrlrDezfYb1tjDFe8yroRWSyiGwUkc0i8ngz2/uIyBIRWS0i60TkMo9tw0XkaxHJEpH17onD271F\nuUWkdIpmYPc4/544y91sM+hy/57XGNNunTDoRSQUZ+7XS4F0YIqIpDfZ7QmcuWRH4Uwe/oL7tWHA\n68DdqjoEuBCo8VnpA6SyxsV/v9nLxMHdEPHjJCP1vW36XWDNNsYYr3lTox8LbFbVrapaDcwHrmqy\njwIJ7uVEoNC9fAmwTlXXAqhqiaq6Wl7sY9jxFRze32qHr7d0awmHa1xM8PewB7vWwr7t1tvGGHNS\nvAn6FGCnx/N89zpPPwemikg+ziTi97vXDwRURBaKyCoReay5E4jIDBHJFJHM4uLik3oDDUq2wNxL\nYdn/nNrrT8Li3CKiw0M567QurX6uRrLeBQmFQVf497zGmHbNVxdjpwAvq2oqcBnwmoiEAGHAucDN\n7sdrRGRi0xer6mxVzVDVjOTk5FMrQZf+cMblsPR5qCw/1fdxQqrKopwizh3Qlajw0FY7TzMndt8k\nZc02xpiT403QFwC9PZ6nutd5uh14C0BVvwaigK44tf/PVXWvqlbg1PZHt7TQx3TBo1BZBstnt9op\nNu05SMH+w/4frbK+2cZ62xhjTpI3Qb8CGCAi/UQkAudi6/tN9skDJgKIyGCcoC8GFgLDRCTGfWH2\nAiDbV4U/Sq9RMHAyfD0Lqg60yikW5e4B4CJ/B332Amu2McackhMGvarWAjNxQjsHp3dNlog8KSLf\ndu/2CHCniKwF5gHT1LEP+APOl8UaYJWq/rs13kiD8x+Dw/tgxYutcvjFOUUMS0mke4Ife4nW3yTV\n73yI9fN1AWNMuxfmzU6q+iFOs4vnup96LGcD5xzjta/jdLH0j9QxcPrF8NVfYOwMiPDdrE+lh6pZ\nlbeP+ycM8NkxvbJ7HezbBuc+6N/zGmOCQnDeGXvBD6GiBFa85NPDfrapiDrF/6NVZtU321zp3/Ma\nY4JCcAZ977Fw2kXw1Z+husJnh12UU0RyfCRDeyX67Jgn1HCT1HnWbGOMOSXBGfTg1OoPFcPKl31y\nuBpXHZ9tKmbCGd0ICfHj3bC710PpVuttY4w5ZcEb9H3HQ9p58OUfoeZwiw+XuX0fBypr/T9aZX1v\nm8HWbGOMOTXBG/Tg1OoP7oFVr7b4UItz9xARGsK5p3f1QcG8VN/bJu1ciPXjeY0xQSW4g77fedD3\nHPjvc1BT2aJDLcot4qz+XYiN9Kqjkm/s2QClW2xsG2NMiwR30ANc8Bgc2AVrTr2H57a9h9hafMj/\nd8NmLQAJsd42xpgWCf6g73cB9B4HXzwHtVWndIjF7klG/DpaZX1vm7RzIe4Ux/8xxhg6QtCLOLX6\n8nxY849TOsTi3D0M6BZH76QYHxfuOPZkQclm621jjGmx4A96gP4TISUDvvgDuE5u3pMDlTUs21oa\noN42ITD42yfe1xhjjqNjBL2I0wOnLA/Wzj+pl37xzV5q69S/c8PW97bpe4412xhjWqxjBD3AgEnQ\ncyR88fuTqtUvyikiMTqc0X06tWLhmijKhpJvrLeNMcYnOk7Q19fq922H9f/06iWuOuXTjUVceEYy\nYaF+/KiyrNnGGOM7HSfoAc64FHoMg89/D67aE+6+Nn8/JYeqA9Pbpu85EOfn6wLGmKDUsYK+vlZf\nugWy/nXC3RfnFBEaIlww0I/t5EU5sHcTpDedf90YY05Nxwp6cOaV7TYEPvst1LmOu+ui3CLG9O1M\np5gIPxUOpzaPWLONMcZnOl7Qh4Q4/epLvoGsd4+5W+H+w+TsKg/M3bB9z4F4P/byMcYENa+CXkQm\ni8hGEdksIo83s72PiCwRkdUisk5ELmtm+0ER+YGvCt4ig78NyYPg899BXV2zu9TfDevXSUaKcmDv\nRuttY4zxqRMGvYiEAs8DlwLpwBQRSW+y2xM4c8mOwpk8/IUm2/8AfNTy4vpISAic/ygU50LOe83u\nsji3iD5JMfRPjvNfubKs2cYY43ve1OjHAptVdauqVgPzgaZXChVIcC8nAoX1G0TkamAbkNXy4vrQ\nkGug60D47Oha/eFqF19u3suEQd0Q8eMkI9kLoO/Z1mxjjPEpb4I+Bdjp8Tzfvc7Tz4GpIpKPM4n4\n/QAiEgf8EPjF8U4gIjNEJFNEMouLi70seguFhDq1+qIs2PjvRpu+2rKXqto6Pzfb5Dq/MGxsG2OM\nj/nqYuwU4GVVTQUuA14TkRCcL4DnVPXg8V6sqrNVNUNVM5KT/diVcci1kNQfPvuN03/dbVFuEbER\noYztl+S/stT3tkm3ZhtjjG95E/QFQG+P56nudZ5uB94CUNWvgSigKzAO+K2IbAceBP6fiMxsYZl9\nJzQMzv+BMy/rRucSgqqyOFLOYXMAABjNSURBVKeI8wYkExkW6r+yZC2APuMhvof/zmmM6RC8CfoV\nwAAR6SciETgXW99vsk8eMBFARAbjBH2xqp6nqmmqmgb8EXhaVWf5rPS+MOw70DmtoVafvauc3eWV\n/h2tsngjFOdYbxtjTKs4YdCrai0wE1gI5OD0rskSkSdFpL6d4RHgThFZC8wDpql6tIW0ZaFhcN4P\nYNca+OZjFuc43SovOsOPQW+9bYwxrUjaWh5nZGRoZmamf0/qqoE/j4a4ZK6uehIV4b37zvHf+V8Y\nD1GJcNt//HdOY0xQEZGVqprR3LaOd2dsc0LD4byHoWAliYWf+/du2OJNzrDE1tvGGNNKLOjrjbyZ\niqgePBD2Lyac4ceeP9bbxhjTyizo64VF8F78jYwJ+YYhVWv8d96sBdDnLEjo5b9zGmM6FAt6t+ra\nOn5bNJay8GSkSb/6VrP3G+eGLWu2Mca0Igt6txXbS9lXJeweehfkfQXb/9v6J81a4Dxas40xphVZ\n0LstyikiMiyEPhffA3HdnX71rS17AfS2ZhtjTOuyoMe5G3ZR7h7O7t+F6Ng4OOdB2P4F7Piq9U66\ndzPs2WA3SRljWp0FPbB17yF2lFQwYbB71Mgx0yA2uXVr9dnuSU/sJiljTCuzoIeGu2EbJgGPiIGz\nH4Ctn0LestY5adZ70HscJDYdCNQYY3zLgh5YlLuHQT3iSekUfWTlmbdDTBf4/Le+P2HJFtiz3nrb\nGGP8osMHfdnhGlZs33f02PMRsXD2/bD5E8hf6duT1s9Vm950/hZjjPG9Dh/0n28qxlWnTBjUzKxO\nZ94B0Z1931afvQBSx1qzjTHGLzp80C/OLSIpNoKRvTsdvTEyHsbfB98shMLVvjlhyRZn/HvrbWOM\n8ZMOHfSuOmXJxiIuPCOZ0JBjzA07doYzsuRnv/PNSbPrb5KyZhtjjH906KBfnbeP/RU1TGyu2aZe\nVCKcdZ8zr+yudS0/adYCSD0TElNbfixjjPFChw76RblFhIUI5w3sevwdx90FkQkt74FTuhV2r7Pe\nNsYYv+rQQb84p4ix/ZJIiAo//o7RnWDc3ZDzAezJOvUTZlmzjTHG/7wKehGZLCIbRWSziDzezPY+\nIrJERFaLyDoRucy9fpKIrBSR9e7HCb5+A6dqZ2kFG/ccOHKT1ImcdQ9ExMPnLWirz3oXUjKgU+8T\n72uMMT5ywqAXkVDgeeBSIB2YIiLpTXZ7Amcu2VE4k4e/4F6/F7hSVYcBtwKv+argLbVko3M37MTB\nx2mf9xSTBONmOLXyotyTP2F9s431tjHG+Jk3NfqxwGZV3aqq1cB8oGnbgwIJ7uVEoBBAVVeraqF7\nfRYQLSKRLS92yy3KKeK0rrH06xrr/YvOug/CY06tVm/NNsaYAPEm6FOAnR7P893rPP0cmCoi+cCH\nwP3NHOc6YJWqVjXdICIzRCRTRDKLi4u9KnhLHKqq5estJd4329SL7QJj74AN7zhzvZ6M7AWQMgY6\n9Tm51xljTAv56mLsFOBlVU0FLgNeE5GGY4vIEOA3wF3NvVhVZ6tqhqpmJCe3/nytX27eS7WrjglN\nhz3wxvj7ITwavvi9968p3Qa71lpvG2NMQHgT9AWA59XDVPc6T7cDbwGo6tdAFNAVQERSgXeBW1R1\nS0sL7AuLc4uIjwzjzLSkk39xXDJk3Abr/+nc5eoNu0nKGBNA3gT9CmCAiPQTkQici63vN9knD5gI\nICKDcYK+WEQ6Af8GHlfVL31X7FNXV6cszi3i/DOSCQ89xR80Zz8AoRHwxbPe7Z+1AHqNhs59T+18\nxhjTAidMOlWtBWYCC4EcnN41WSLypIjUz5rxCHCniKwF5gHTVFXdrzsd+KmIrHH/nUJ7ie9kFZZT\ndKCKiSfbPu8pvjuMmQ5r5zvNMsdTug12rbHeNsaYgAnzZidV/RDnIqvnup96LGcD5zTzuqeAp1pY\nRp9alLsHEbjwjBZ+35zzfcic49Tqr5p17P2y33MerdnGGBMgHe7O2MW5RYzu05mk2IiWHSihJ4y5\nFdbOg307jr1f9gLoNQo6p7XsfMYYc4o6VNAXlVeyLr/s5LtVHss5D4KEwH+fa377vu3O8MbW28YY\nE0AdKuiP3A3ro6BPTIFRU2H167B/59Hb65ttrH3eGBNAHSroF+UUkdIpmjO6x/vuoOc+7Dx++cej\nt2UtgJ4jrdnGGBNQXl2MDQaVNS7+u3kv141OReQYk4ycik69YeR3YdWrcN4jkNDLWb9vBxSugot/\n7rtzGeNjNTU15OfnU1lZGeiiGC9FRUWRmppKePgJRt310GGCftm2UiqqXad2N+yJnPew03zz5Z/g\nUvf8sg29bazZxrRd+fn5xMfHk5aW5tsKkGkVqkpJSQn5+fn069fP69d1mKabxTl7iA4PZfxpXXx/\n8M5pMGIKrHwZDux21mUvgJ4jIMn7/xjG+FtlZSVdunSxkG8nRIQuXbqc9C+wDhH0qsqi3CLOOb0r\nUeGhrXOS8x8BVw18+WfYnwcFK602b9oFC/n25VT+e3WIpptvig6Sv+8w9110euudJOk0GP4d5yaq\nUPfHar1tjDFtQIeo0S/KcbpVXtTSu2FP5LxHwFXltNX3GO6EvzHmmEpKShg5ciQjR46kR48epKSk\nNDyvrq726hjTp09n48aNJ33uK664gnPPPfekX9cedYga/eLcPQzplUCPxKjWPVHXATD0OmdkS6vN\nG3NCXbp0Yc2aNQD8/Oc/Jy4ujh/84AeN9lFVVJWQkObrpXPnzj3p85aWlrJu3TqioqLIy8ujT5/W\nmSeitraWsLDAx2zgS9DK9h2qZuWOfcxszWYbTxf9PzhY5FycNaYd+cUHWWQXlvv0mOm9EvjZlUNO\n+nWbN2/m29/+NqNGjWL16tV8/PHH/OIXv2DVqlUcPnyYG2+8kZ/+1Blu69xzz2XWrFkMHTqUrl27\ncvfdd/PRRx8RExPDe++9R7duR/+Sf/vtt7n66qtJTExk/vz5PPbYYwDs3r2bu+66i23btiEizJ49\nm3HjxjF37lyee+45RITRo0czd+5cpk6dyvXXX8/VVzuVuri4OA4ePMgnn3zCU089RVxcHFu2bCEn\nJ4crr7ySwsJCKisreeihh7jjjjsA+Pe//81PfvITXC4X3bt35z//+Q8DBw5k+fLlJCUl4XK5GDBg\nAJmZmSQlncKw6m5BH/SfbSqmTmGCt3PDtlTSaXBr01GcjTEnKzc3l1dffZWMjAwAnnnmGZKSkqit\nreWiiy7i+uuvJz298fTVZWVlXHDBBTzzzDM8/PDDzJkzh8cff/yoY8+bN4+nn36axMREbr755oag\nv++++5g0aRIzZ86ktraWiooK1q5dy29+8xu++uorkpKSKC0tPWHZMzMzyc7Obvil8Morr5CUlERF\nRQUZGRlcd911VFVVcc899/DFF1/Qt29fSktLCQkJYcqUKfzjH/9g5syZLFy4kDPPPLNFIQ8dIOgX\n5RbRNS6C4SmJgS6KMW3aqdS8W1P//v0bQh6ccH7ppZeora2lsLCQ7Ozso4I+OjqaSy+9FIAxY8bw\nxRdfHHXcwsJC8vLyGD9+PAB1dXXk5uYyaNAgPv30U+bPnw9AWFgYCQkJLF68mBtvvLEhbL0J3fHj\nxzdqDnruued4/32nApifn8+WLVvYuXMnF110EX379m103Ntvv50bbriBmTNnMmfOnIbaf0sE9cXY\nGlcdn20s4qIzuhESYl3IjGlPYmNjG5a/+eYb/vSnP7F48WLWrVvH5MmTm+1LHhFxZFTa0NBQamtr\nj9rnzTffZO/evaSlpZGWlkZeXh7z5s1r2O5t98WwsDDq6uoAcLlcjc7lWfZPPvmEzz//nKVLl7J2\n7VqGDx9+3H7waWlpdO7cmSVLlrB69WouueQSr8pzPEEd9Ct37KO8stZ3g5gZYwKivLyc+Ph4EhIS\n2LVrFwsXLjzlY82bN49PPvmE7du3s337dpYvX94Q9BdddBF/+9vfACe8y8vLmTBhAm+++WZDk039\nY1paGitXrgTg3XffxeVyNXu+srIykpKSiI6OJisrixUrVgBw9tlns2TJEnbs2NHouODU6m+++WZu\nuummY16EPhleHUFEJovIRhHZLCJHNXiJSB8RWSIiq0VknYhc5rHtR+7XbRSRb7W4xCdhcW4R4aHC\nuQNaf8JxY0zrGT16NOnp6QwaNIhbbrmFc845ap4jr2zZsoVdu3Y1ahIaMGAAUVFRrFy5klmzZrFw\n4UKGDRtGRkYGubm5jBgxgscee4zzzz+fkSNH8uijjwJw11138fHHHzNixAhWr15NZGRks+e8/PLL\nqaioID09nSeeeIJx48YB0L17d/76179y1VVXMWLECG6++eaG11xzzTWUlZUxbdq0U3qfTYkz499x\ndhAJBTYBk4B8nDlkp7hnlarfZzawWlX/KiLpwIeqmuZengeMBXoBnwADVbX5rz4gIyNDMzMzW/i2\nHBOf/ZRenaJ57fZxPjmeMcEmJyeHwYMHB7oYpomlS5fyox/9iCVLljS7vbn/biKyUlUzmtvfmxr9\nWGCzqm5V1WpgPtB0XjwFEtzLiUChe/kqYL6qVqnqNmCz+3itbvveQ2wpPuS7SUaMMcYPfvWrX3Hj\njTfy9NNP++yY3gR9CuA5q0a+e52nnwNTRSQfZ27Z+0/ita1ica5zN6wFvTGmPfnxj3/Mjh07GnoF\n+YKvLsZOAV5W1VTgMuA1EfH62CIyQ0QyRSSzuLjYJwVanFvE6d3i6Nsl9sQ7G2NMEPMmjAuA3h7P\nU93rPN0OvAWgql8DUUBXL1+Lqs5W1QxVzUhObvmF0wOVNSzbVsJEq80bY4xXQb8CGCAi/UQkArgJ\naHrrZx4wEUBEBuMEfbF7v5tEJFJE+gEDgOW+Kvyx/PebvdS41JptjDEGL+6MVdVaEZkJLARCgTmq\nmiUiTwKZqvo+8AjwdxF5COfC7DR1uvNkichbQDZQC9x3vB43vrIot4iEqDDG9O3c2qcyxpg2z6sh\nEFT1Q5yLrJ7rfuqxnA0027FVVX8F/KoFZTwpdXXKktwiLjyjG2GhQX0/mDHtXklJCRMnTgScAcVC\nQ0Opb75dvnx5oztdj2fOnDlcdtll9OjRo9nt1dXV9OjRg3vvvZennnrKN4VvR4IuCdfm76fkULXd\nDWtMO1A/TPGaNWu4++67eeihhxqeexvy4AT97t27j7l94cKFpKen8+abb/qi2MfU3JALbUHQDWq2\nOLeIEIELBtrdsMaclI8eh93rfXvMHsPg0mdO6aWvvPIKzz//PNXV1Zx99tnMmjWLuro6pk+fzpo1\na1BVZsyYQffu3VmzZg033ngj0dHRzf4SmDdvHg8//DDPPfccy5cvZ+xY53aeZcuW8eCDD1JRUUFU\nVBRLliwhIiKCRx99lI8//piQkBDuvvtu7r33XlJTU9mwYQOdOnVi6dKlPPHEE3zyySc88cQT5OXl\nsWXLFvr168cvfvELpk2bxsGDBwkJCeGFF15ouBv26aefZt68eYSEhHDFFVdwyy23MHXq1IZhEXJy\ncrj11ltZvty3lzKDLugX5RSR0TeJTjHe1waMMW3Lhg0bePfdd/nqq68ICwtjxowZzJ8/n/79+7N3\n717Wr3e+kPbv30+nTp34y1/+wqxZsxg5cuRRx6qoqODTTz9tqPXPmzePsWPHUllZyU033cQ777zD\n6NGjKSsrIzIykhdeeIHCwkLWrl1LaGioV8MS5+bm8vnnnxMVFUVFRQUff/wxUVFR5Obmcuutt7Js\n2TI++OADPvroI5YvX050dDSlpaUNY+Bs2LCBoUOHMnfuXKZPn+7zzzOogn5X2WGyd5Xz+KWDAl0U\nY9qfU6x5t4ZPPvmEFStWNIxJc/jwYXr37s23vvUtNm7cyAMPPMDll1/u1ciO77//PpMmTSIqKoob\nbriBMWPG8Oyzz5KTk0OfPn0YPXo0AImJiQ3nfvDBBwkNDQW8G5b4qquuIirKmcGuqqqKmTNnsnbt\nWsLCwtiyZUvDcW+77Taio6MbHff2229n7ty5/OY3v+Gf//wnq1evPpmPyitBFfT1d8Na/3lj2jdV\n5bbbbuOXv/zlUdvWrVvHRx99xPPPP88777zD7Nmzj3usefPmsXTpUtLS0gAoLi7ms88+o1OnTidV\nJs9hiZsOM+w5LPGzzz5L7969ef3116mpqSEuLu64x73hhht4+umnOeeccxg/fvxJl8sbQXUxdklu\nEb2Tojm92/E/WGNM23bxxRfz1ltvsXfvXsDpnZOXl0dxcTGqyg033MCTTz7JqlWrAIiPj+fAgQNH\nHWf//v0sXbqU/Pz8hmGJ//znPzNv3jzS09PJy8trOEZ5eTkul4tJkybxt7/9rWHY4eaGJX7nnXeO\nWfaysjJ69uyJiPDKK69QP3DkpEmTmDNnDocPH2503JiYGCZMmMDMmTNbpdkGgijoK2tc/HfzXiYO\n6u71xAHGmLZp2LBh/OxnP+Piiy9m+PDhXHLJJezZs4edO3c2DBc8ffr0hoG/pk+fzh133MHIkSOp\nrq5uOM4777zDpEmTCA8Pb1h39dVXs2DBAkJCQpg3bx733HMPI0aM4JJLLqGqqoq77rqLHj16MHz4\ncEaMGMFbb70FOJOX33vvvZx55pnH7RE0c+ZMXnzxRUaMGMG2bdsahi++4oormDx5MhkZGYwcOZLn\nnnuu4TU333wz4eHhDV1Nfe2EwxT726kOU1xUXslT/85hytg+jO/fpRVKZkzwsWGK24ZnnnmGqqoq\nfvazn3m1/8kOUxw0bfTdEqL485RRgS6GMcaclCuvvJKdO3eyePHiVjtH0AS9Mca0Rx988EGrnyNo\n2uiNMaemrTXfmuM7lf9eFvTGdGBRUVGUlJRY2LcTqkpJSUlDn31vWdONMR1Yamoq+fn5+GrCH9P6\noqKiSE1NPanXWNAb04GFh4fTr1+/QBfDtDJrujHGmCBnQW+MMUHOgt4YY4Jcm7szVkSKgR0tOERX\nYK+PitPe2WfRmH0ejdnncUQwfBZ9VbXZiTjaXNC3lIhkHus24I7GPovG7PNozD6PI4L9s7CmG2OM\nCXIW9MYYE+SCMeiPPwtBx2KfRWP2eTRmn8cRQf1ZBF0bvTHGmMaCsUZvjDHGgwW9McYEuaAJehGZ\nLCIbRWSziDwe6PIEkoj0FpElIpItIlki8v1AlynQRCRURFaLyP8GuiyBJiKdRORtEckVkRwRGR/o\nMgWSiDzk/neyQUTmicjJDQ3ZDgRF0ItIKPA8cCmQDkwRkfTAliqgaoFHVDUdOAu4r4N/HgDfB3IC\nXYg24k/Af1R1EDCCDvy5iEgK8ACQoapDgVDgpsCWyveCIuiBscBmVd2qqtXAfOCqAJcpYFR1l6qu\nci8fwPmHnBLYUgWOiKQClwMvBrosgSYiicD5wEsAqlqtqvsDW6qACwOiRSQMiAEKA1wenwuWoE8B\ndno8z6cDB5snEUkDRgHLAluSgPoj8BhQF+iCtAH9gGJgrrsp60URiQ10oQJFVQuA3wN5wC6gTFX/\nL7Cl8r1gCXrTDBGJA94BHlTV8kCXJxBE5AqgSFVXBrosbUQYMBr4q6qOAg4BHfaaloh0xvn13w/o\nBcSKyNTAlsr3giXoC4DeHs9T3es6LBEJxwn5N1T1X4EuTwCdA3xbRLbjNOlNEJHXA1ukgMoH8lW1\n/hfe2zjB31FdDGxT1WJVrQH+BZwd4DL5XLAE/QpggIj0E5EInIsp7we4TAEjIoLTBpujqn8IdHkC\nSVV/pKqpqpqG8//FYlUNuhqbt1R1N7BTRM5wr5oIZAewSIGWB5wlIjHufzcTCcKL00ExlaCq1orI\nTGAhzlXzOaqaFeBiBdI5wPeA9SKyxr3u/6nqhwEsk2k77gfecFeKtgLTA1yegFHVZSLyNrAKp7fa\naoJwOAQbAsEYY4JcsDTdGGOMOQYLemOMCXIW9MYYE+Qs6I0xJshZ0BtjTJCzoDcdkoi4RGSNx5/P\n7g4VkTQR2eCr4xnTUkHRj96YU3BYVUcGuhDG+IPV6I3xICLbReS3IrJeRJaLyOnu9WkislhE1onI\nIhHp417fXUTeFZG17r/62+dDReTv7nHO/09EogP2pkyHZ0FvOqroJk03N3psK1PVYcAsnJEvAf4C\nvKKqw4E3gD+71/8Z+ExVR+CMGVN/R/YA4HlVHQLsB65r5fdjzDHZnbGmQxKRg6oa18z67cAEVd3q\nHhhut6p2EZG9QE9VrXGv36WqXUWkGEhV1SqPY6QBH6vqAPfzHwLhqvpU678zY45mNXpjjqbHWD4Z\nVR7LLux6mAkgC3pjjnajx+PX7uWvODLF3M3AF+7lRcA90DAvbaK/CmmMt6yWYTqqaI+RPcGZQ7W+\ni2VnEVmHUyuf4l53P86sTI/izNBUP+Lj94HZInI7Ts39HpyZioxpM6yN3hgP7jb6DFXdG+iyGOMr\n1nRjjDFBzmr0xhgT5KxGb4wxQc6C3hhjgpwFvTHGBDkLemOMCXIW9MYYE+T+P78sp02gIdUZAAAA\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GT2dRPM19Kok",
        "colab_type": "text"
      },
      "source": [
        "And confusion matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UWztNslAwmO",
        "colab_type": "code",
        "outputId": "5a6d95c0-62a0-4eda-cf9f-0baa6a4e77b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "pd.DataFrame(res['conf_matr'], index=['pred pos', 'pred neg'], columns=['actual pos', 'actual neg'])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>actual pos</th>\n",
              "      <th>actual neg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>pred pos</th>\n",
              "      <td>2385</td>\n",
              "      <td>232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred neg</th>\n",
              "      <td>344</td>\n",
              "      <td>2763</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          actual pos  actual neg\n",
              "pred pos        2385         232\n",
              "pred neg         344        2763"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXjXyLARfRsM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "41341a12-0532-48d7-8a12-b2d65c3a897c"
      },
      "source": [
        "pd.DataFrame([float(res['f1']),\n",
        "              float(res['precision']), \n",
        "              float(res['recall'])], \n",
        "              index = ['f1 score', 'precision', 'recall'],\n",
        "              columns=[''])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>f1 score</th>\n",
              "      <td>0.892256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>precision</th>\n",
              "      <td>0.911349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recall</th>\n",
              "      <td>0.873946</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   \n",
              "f1 score   0.892256\n",
              "precision  0.911349\n",
              "recall     0.873946"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    }
  ]
}