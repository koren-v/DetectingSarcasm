{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Copy of bert_pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbGbIZA5E0u8",
        "colab_type": "code",
        "outputId": "5f197423-e5fe-4c0b-cbce-5b6efe7a5aa0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1J4V7cYeEs2V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp drive/'My Drive'/BERT/Sarcasm_Headlines_Dataset_v2.json ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fP0gcU3pHDEc",
        "colab_type": "code",
        "outputId": "40b4c101-32b3-409a-d294-60d7758c1448",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        }
      },
      "source": [
        "!pip install pytorch-pretrained-bert"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-pretrained-bert in /usr/local/lib/python3.6/dist-packages (0.6.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.17.4)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2019.12.9)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.28.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.10.40)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.21.0)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.40 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.13.40)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.2.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.40->boto3->pytorch-pretrained-bert) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.40->boto3->pytorch-pretrained-bert) (2.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\"->botocore<1.14.0,>=1.13.40->boto3->pytorch-pretrained-bert) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHHVoxcGEq2m",
        "colab_type": "code",
        "outputId": "21c36eaa-a62e-41fe-cc50-2003e0b93179",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import torch\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
        "\n",
        "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmNBPdvlEq2t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from random import randrange\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_lGSB6iEq3W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertLayerNorm(nn.Module):\n",
        "        def __init__(self, hidden_size, eps=1e-12):\n",
        "            \"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n",
        "            \"\"\"\n",
        "            super(BertLayerNorm, self).__init__()\n",
        "            self.weight = nn.Parameter(torch.ones(hidden_size))\n",
        "            self.bias = nn.Parameter(torch.zeros(hidden_size))\n",
        "            self.variance_epsilon = eps\n",
        "\n",
        "        def forward(self, x):\n",
        "            u = x.mean(-1, keepdim=True)\n",
        "            s = (x - u).pow(2).mean(-1, keepdim=True)\n",
        "            x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
        "            return self.weight * x + self.bias\n",
        "        \n",
        "\n",
        "class BertForSequenceClassification(nn.Module):\n",
        "    \"\"\"BERT model for classification.\n",
        "    This module is composed of the BERT model with a linear layer on top of\n",
        "    the pooled output.\n",
        "    Params:\n",
        "        `config`: a BertConfig class instance with the configuration to build a new model.\n",
        "        `num_labels`: the number of classes for the classifier. Default = 2.\n",
        "    Inputs:\n",
        "        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
        "            with the word token indices in the vocabulary. Items in the batch should begin with the special \"CLS\" token. (see the tokens preprocessing logic in the scripts\n",
        "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
        "        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
        "            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
        "            a `sentence B` token (see BERT paper for more details).\n",
        "        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
        "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
        "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
        "            a batch has varying length sentences.\n",
        "        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size]\n",
        "            with indices selected in [0, ..., num_labels].\n",
        "    Outputs:\n",
        "        if `labels` is not `None`:\n",
        "            Outputs the CrossEntropy classification loss of the output with the labels.\n",
        "        if `labels` is `None`:\n",
        "            Outputs the classification logits of shape [batch_size, num_labels].\n",
        "    Example usage:\n",
        "    ```python\n",
        "    # Already been converted into WordPiece token ids\n",
        "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
        "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
        "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
        "    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
        "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
        "    num_labels = 2\n",
        "    model = BertForSequenceClassification(config, num_labels)\n",
        "    logits = model(input_ids, token_type_ids, input_mask)\n",
        "    ```\n",
        "    \"\"\"\n",
        "    def __init__(self, config, num_labels=2):\n",
        "        super(BertForSequenceClassification, self).__init__()\n",
        "        self.num_labels = num_labels\n",
        "        self.config = config\n",
        "        #self.bert = BertModel(config=config) # if you want to do castom config\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
        "        nn.init.xavier_normal_(self.classifier.weight)\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
        "        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        return logits\n",
        "    def freeze_bert_encoder(self):\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = False\n",
        "    \n",
        "    def unfreeze_bert_encoder(self):\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oa-lw9IVEq3a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pytorch_pretrained_bert import BertConfig\n",
        "\n",
        "config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
        "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
        "\n",
        "num_labels = 2\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNG4bwXXgtor",
        "colab_type": "code",
        "outputId": "d750cf73-b2f5-40f4-b9fc-174530b119c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "source": [
        "model = BertForSequenceClassification(config, num_labels)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:pytorch_pretrained_bert.modeling:loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
            "INFO:pytorch_pretrained_bert.modeling:extracting archive file /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmp7sztr30d\n",
            "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_u2XuqREEq3g",
        "colab_type": "code",
        "outputId": "a218b118-8c30-440a-fb7c-4c867bf88cd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_json(\"Sarcasm_Headlines_Dataset_v2.json\", lines=True)\n",
        "df.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>is_sarcastic</th>\n",
              "      <th>headline</th>\n",
              "      <th>article_link</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>thirtysomething scientists unveil doomsday clo...</td>\n",
              "      <td>https://www.theonion.com/thirtysomething-scien...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>dem rep. totally nails why congress is falling...</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/donna-edw...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>eat your veggies: 9 deliciously different recipes</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/eat-your-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>inclement weather prevents liar from getting t...</td>\n",
              "      <td>https://local.theonion.com/inclement-weather-p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>mother comes pretty close to using word 'strea...</td>\n",
              "      <td>https://www.theonion.com/mother-comes-pretty-c...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   is_sarcastic  ...                                       article_link\n",
              "0             1  ...  https://www.theonion.com/thirtysomething-scien...\n",
              "1             0  ...  https://www.huffingtonpost.com/entry/donna-edw...\n",
              "2             0  ...  https://www.huffingtonpost.com/entry/eat-your-...\n",
              "3             1  ...  https://local.theonion.com/inclement-weather-p...\n",
              "4             1  ...  https://www.theonion.com/mother-comes-pretty-c...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94YCTwJLH1RS",
        "colab_type": "code",
        "outputId": "38d281df-eceb-479f-9786-0a677f655bd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "df = df.drop(['article_link'], axis=1)\n",
        "df.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>is_sarcastic</th>\n",
              "      <th>headline</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>thirtysomething scientists unveil doomsday clo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>dem rep. totally nails why congress is falling...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>eat your veggies: 9 deliciously different recipes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>inclement weather prevents liar from getting t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>mother comes pretty close to using word 'strea...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   is_sarcastic                                           headline\n",
              "0             1  thirtysomething scientists unveil doomsday clo...\n",
              "1             0  dem rep. totally nails why congress is falling...\n",
              "2             0  eat your veggies: 9 deliciously different recipes\n",
              "3             1  inclement weather prevents liar from getting t...\n",
              "4             1  mother comes pretty close to using word 'strea..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvSxEUK2Eq4H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X = df['headline']\n",
        "y = df['is_sarcastic']\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOX0oLliEq4O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.tolist()\n",
        "X_test = X_test.tolist()\n",
        "\n",
        "y_train = y_train.tolist()\n",
        "y_test = y_test.tolist()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBCLwuUjEq4T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "max_seq_length = 24\n",
        "class text_dataset(Dataset):\n",
        "    def __init__(self,x_y_list, transform=None):\n",
        "        \n",
        "        self.x_y_list = x_y_list\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __getitem__(self,index):\n",
        "        \n",
        "        tokenized_review = tokenizer.tokenize(self.x_y_list[0][index])\n",
        "        \n",
        "        if len(tokenized_review) > max_seq_length:\n",
        "            tokenized_review = tokenized_review[:max_seq_length]\n",
        "            \n",
        "        ids_review  = tokenizer.convert_tokens_to_ids(tokenized_review)\n",
        "\n",
        "        padding = [0] * (max_seq_length - len(ids_review))\n",
        "        \n",
        "        ids_review += padding\n",
        "        \n",
        "        assert len(ids_review) == max_seq_length\n",
        "        \n",
        "        #print(ids_review)\n",
        "        ids_review = torch.tensor(ids_review)\n",
        "        \n",
        "        sentiment = self.x_y_list[1][index] # color        \n",
        "        list_of_labels = [torch.from_numpy(np.array(sentiment))]\n",
        "        \n",
        "        \n",
        "        return ids_review, list_of_labels[0]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.x_y_list[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3-3W5cSEq4V",
        "colab_type": "code",
        "outputId": "601b7759-fc5e-41f0-f53a-5d95e4739320",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_lists = [X_train, y_train]\n",
        "test_lists = [X_test, y_test]\n",
        "\n",
        "training_dataset = text_dataset(x_y_list = train_lists )\n",
        "\n",
        "test_dataset = text_dataset(x_y_list = test_lists )\n",
        "\n",
        "dataloaders_dict = {'train': torch.utils.data.DataLoader(training_dataset, batch_size=batch_size, shuffle=True, num_workers=0),\n",
        "                   'val':torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "                   }\n",
        "dataset_sizes = {'train':len(train_lists[0]),\n",
        "                'val':len(test_lists[0])}\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vteX76PFEq4Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "    print('starting')\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss = 100\n",
        "\n",
        "    val_loss = []\n",
        "    train_loss = []\n",
        "    val_acc = []\n",
        "    train_acc = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0          \n",
        "            sentiment_corrects = 0\n",
        "            tp = 0.0\n",
        "            tn = 0.0\n",
        "            fp = 0.0\n",
        "            fn = 0.0\n",
        "            \n",
        "            # Iterate over data.\n",
        "            for inputs, sentiment in dataloaders_dict[phase]:\n",
        "\n",
        "                inputs = inputs.to(device) \n",
        "                sentiment = sentiment.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "\n",
        "                    outputs = model(inputs)\n",
        "                    outputs = F.softmax(outputs,dim=1)\n",
        "\n",
        "                    loss = criterion(outputs, sentiment)\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        \n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "                sentiment_corrects += torch.sum(torch.max(outputs, 1)[1] == sentiment)\n",
        "\n",
        "                tp += torch.sum(torch.max(outputs, 1)[1] & sentiment)\n",
        "                tn += torch.sum(1-torch.max(outputs, 1)[1] & 1-sentiment)\n",
        "                fp += torch.sum(torch.max(outputs, 1)[1] & 1-sentiment)\n",
        "                fn += torch.sum(1-torch.max(outputs, 1)[1] & sentiment)\n",
        "\n",
        "                \n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "\n",
        "            sentiment_acc = sentiment_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            if phase == 'train':\n",
        "                train_acc.append(sentiment_acc)\n",
        "                train_loss.append(epoch_loss)\n",
        "            elif phase == 'val':\n",
        "                val_acc.append(sentiment_acc)\n",
        "                val_loss.append(epoch_loss)\n",
        "\n",
        "            print('{} total loss: {:.4f} '.format(phase,epoch_loss ))\n",
        "            print('{} sentiment_acc: {:.4f}'.format(\n",
        "                phase, sentiment_acc))\n",
        "\n",
        "            if phase == 'val' and epoch_loss < best_loss:\n",
        "                print('saving with loss of {}'.format(epoch_loss),\n",
        "                      'improved over previous {}'.format(best_loss))\n",
        "                best_loss = epoch_loss\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                torch.save(model.state_dict(), 'bert_model_test.pth')\n",
        "\n",
        "            if phase == 'val' and epoch == num_epochs - 1:\n",
        "                recall = tp / (tp + fn)\n",
        "                print('recall {:.2f}'.format(recall))\n",
        "\n",
        "        print()\n",
        "    \n",
        "    confusion_matrix = [[tp, fp],[fn, tn]]\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val loss: {:4f}'.format(float(best_loss)))\n",
        "\n",
        "    results = {'time': time_elapsed, 'recall': recall, 'conf_matr': confusion_matrix, 'val_loss': val_loss, 'train_loss': train_loss, 'val_acc': val_acc, 'train_acc': train_acc} \n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Bpdj3zFEq4b",
        "colab_type": "code",
        "outputId": "c1a2a6e7-4411-4691-99cd-f33786fb6e18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.to(device)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpirB1t1Eq4d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lrlast = .001\n",
        "lrmain = .00001\n",
        "optim1 = optim.Adam(\n",
        "    [\n",
        "        {\"params\":model.bert.parameters(),\"lr\": lrmain},\n",
        "        {\"params\":model.classifier.parameters(), \"lr\": lrlast},\n",
        "       \n",
        "   ])\n",
        "\n",
        "#optim1 = optim.Adam(model.parameters(), lr=0.001)#,momentum=.9)\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim1\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=4, gamma=0.1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "rDOkYH2_Eq4h",
        "colab_type": "code",
        "outputId": "9edb9f17-77d7-4248-cd38-52edaf699219",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model_ft1, res = train_model(model, criterion, optimizer_ft, exp_lr_scheduler,\n",
        "                       num_epochs=8)\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "starting\n",
            "Epoch 1/8\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train total loss: 0.5124 \n",
            "train sentiment_acc: 0.7863\n",
            "val total loss: 0.4987 \n",
            "val sentiment_acc: 0.8075\n",
            "saving with loss of 0.49869834066770197 improved over previous 100\n",
            "\n",
            "Epoch 2/8\n",
            "----------\n",
            "train total loss: 0.4296 \n",
            "train sentiment_acc: 0.8795\n",
            "val total loss: 0.4191 \n",
            "val sentiment_acc: 0.8889\n",
            "saving with loss of 0.4191204903427327 improved over previous 0.49869834066770197\n",
            "\n",
            "Epoch 3/8\n",
            "----------\n",
            "train total loss: 0.4054 \n",
            "train sentiment_acc: 0.9052\n",
            "val total loss: 0.4132 \n",
            "val sentiment_acc: 0.8948\n",
            "saving with loss of 0.4132066796410759 improved over previous 0.4191204903427327\n",
            "\n",
            "Epoch 4/8\n",
            "----------\n",
            "train total loss: 0.3825 \n",
            "train sentiment_acc: 0.9284\n",
            "val total loss: 0.4070 \n",
            "val sentiment_acc: 0.9029\n",
            "saving with loss of 0.4069777218342494 improved over previous 0.4132066796410759\n",
            "\n",
            "Epoch 5/8\n",
            "----------\n",
            "train total loss: 0.3778 \n",
            "train sentiment_acc: 0.9330\n",
            "val total loss: 0.4097 \n",
            "val sentiment_acc: 0.8997\n",
            "\n",
            "Epoch 6/8\n",
            "----------\n",
            "train total loss: 0.3744 \n",
            "train sentiment_acc: 0.9366\n",
            "val total loss: 0.4095 \n",
            "val sentiment_acc: 0.8999\n",
            "\n",
            "Epoch 7/8\n",
            "----------\n",
            "train total loss: 0.3722 \n",
            "train sentiment_acc: 0.9399\n",
            "val total loss: 0.4044 \n",
            "val sentiment_acc: 0.9055\n",
            "saving with loss of 0.4043640316282428 improved over previous 0.4069777218342494\n",
            "\n",
            "Epoch 8/8\n",
            "----------\n",
            "train total loss: 0.3690 \n",
            "train sentiment_acc: 0.9425\n",
            "val total loss: 0.4051 \n",
            "val sentiment_acc: 0.9057\n",
            "recall 0.89\n",
            "\n",
            "Training complete in 12m 15s\n",
            "Best val loss: 0.404364\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wpZQojLEq42",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsRQeciGrQ7R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 656
        },
        "outputId": "8b0117f7-8f71-41b3-9cf6-b4b9d9ce186e"
      },
      "source": [
        "res\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'conf_matr': [[tensor(2437., device='cuda:0'), tensor(248., device='cuda:0')],\n",
              "  [tensor(292., device='cuda:0'), tensor(2747., device='cuda:0')]],\n",
              " 'recall': tensor(0.8930, device='cuda:0'),\n",
              " 'time': 735.4491994380951,\n",
              " 'train_acc': [tensor(0.7863, device='cuda:0', dtype=torch.float64),\n",
              "  tensor(0.8795, device='cuda:0', dtype=torch.float64),\n",
              "  tensor(0.9052, device='cuda:0', dtype=torch.float64),\n",
              "  tensor(0.9284, device='cuda:0', dtype=torch.float64),\n",
              "  tensor(0.9330, device='cuda:0', dtype=torch.float64),\n",
              "  tensor(0.9366, device='cuda:0', dtype=torch.float64),\n",
              "  tensor(0.9399, device='cuda:0', dtype=torch.float64),\n",
              "  tensor(0.9425, device='cuda:0', dtype=torch.float64)],\n",
              " 'train_loss': [0.5123965606467644,\n",
              "  0.4295772046130618,\n",
              "  0.4053513043401647,\n",
              "  0.38251368983991024,\n",
              "  0.3778434801099081,\n",
              "  0.374391810154389,\n",
              "  0.3721564257761932,\n",
              "  0.3689916972328831],\n",
              " 'val_acc': [tensor(0.8075, device='cuda:0', dtype=torch.float64),\n",
              "  tensor(0.8889, device='cuda:0', dtype=torch.float64),\n",
              "  tensor(0.8948, device='cuda:0', dtype=torch.float64),\n",
              "  tensor(0.9029, device='cuda:0', dtype=torch.float64),\n",
              "  tensor(0.8997, device='cuda:0', dtype=torch.float64),\n",
              "  tensor(0.8999, device='cuda:0', dtype=torch.float64),\n",
              "  tensor(0.9055, device='cuda:0', dtype=torch.float64),\n",
              "  tensor(0.9057, device='cuda:0', dtype=torch.float64)],\n",
              " 'val_loss': [0.49869834066770197,\n",
              "  0.4191204903427327,\n",
              "  0.4132066796410759,\n",
              "  0.4069777218342494,\n",
              "  0.40971197542987614,\n",
              "  0.40945453874136833,\n",
              "  0.4043640316282428,\n",
              "  0.4051259880629059]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8THlZmAt8uab",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "939e0cac-5041-4938-f5e6-16a56887a755"
      },
      "source": [
        "from matplotlib.pyplot import plot\n",
        "plot(res['train_acc'])\n",
        "plot(res['val_acc'])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f97292bdf60>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOAt6Y4yJcP8foCGY1yTRZpEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UWztNslAwmO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ab6529fd-7830-4189-bed7-a7275d4134ba"
      },
      "source": [
        "res['conf_matr']"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[tensor(2437., device='cuda:0'), tensor(248., device='cuda:0')],\n",
              " [tensor(292., device='cuda:0'), tensor(2747., device='cuda:0')]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXjXyLARfRsM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}