{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "BERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbGbIZA5E0u8",
        "colab_type": "code",
        "outputId": "96cdfbf0-9d57-4786-dd4e-20f3325d4aac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1J4V7cYeEs2V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !cp drive/'My Drive'/BERT/Sarcasm_Headlines_Dataset_v2.json ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fP0gcU3pHDEc",
        "colab_type": "code",
        "outputId": "7eb6f46a-d247-47b7-91ae-b6924e44dd0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        }
      },
      "source": [
        "!pip install pytorch-pretrained-bert"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-pretrained-bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 30.5MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 2.1MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 2.0MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 2.3MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 3.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 3.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.17.4)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.10.40)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.28.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.21.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2019.12.9)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.3.1)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.2.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.40 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.13.40)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.40->boto3->pytorch-pretrained-bert) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.40->boto3->pytorch-pretrained-bert) (2.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\"->botocore<1.14.0,>=1.13.40->boto3->pytorch-pretrained-bert) (1.12.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHHVoxcGEq2m",
        "colab_type": "code",
        "outputId": "9c7ced80-a90e-4b41-cda5-abffb73820e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "import torch\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
        "\n",
        "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:pytorch_pretrained_bert.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache, downloading to /tmp/tmplv8bscvx\n",
            "100%|██████████| 231508/231508 [00:00<00:00, 2725158.72B/s]\n",
            "INFO:pytorch_pretrained_bert.file_utils:copying /tmp/tmplv8bscvx to cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "INFO:pytorch_pretrained_bert.file_utils:creating metadata file for /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "INFO:pytorch_pretrained_bert.file_utils:removing temp file /tmp/tmplv8bscvx\n",
            "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmNBPdvlEq2t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from random import randrange\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from pytorch_pretrained_bert import BertConfig\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_lGSB6iEq3W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertLayerNorm(nn.Module):\n",
        "        def __init__(self, hidden_size, eps=1e-12):\n",
        "            \"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n",
        "            \"\"\"\n",
        "            super(BertLayerNorm, self).__init__()\n",
        "            self.weight = nn.Parameter(torch.ones(hidden_size))\n",
        "            self.bias = nn.Parameter(torch.zeros(hidden_size))\n",
        "            self.variance_epsilon = eps\n",
        "\n",
        "        def forward(self, x):\n",
        "            u = x.mean(-1, keepdim=True)\n",
        "            s = (x - u).pow(2).mean(-1, keepdim=True)\n",
        "            x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
        "            return self.weight * x + self.bias\n",
        "        \n",
        "\n",
        "class BertForSequenceClassification(nn.Module):\n",
        "    \"\"\"BERT model for classification.\n",
        "    This module is composed of the BERT model with a linear layer on top of\n",
        "    the pooled output.\n",
        "    Params:\n",
        "        `config`: a BertConfig class instance with the configuration to build a new model.\n",
        "        `num_labels`: the number of classes for the classifier. Default = 2.\n",
        "    Inputs:\n",
        "        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
        "            with the word token indices in the vocabulary. Items in the batch should begin with the special \"CLS\" token. (see the tokens preprocessing logic in the scripts\n",
        "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
        "        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
        "            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
        "            a `sentence B` token (see BERT paper for more details).\n",
        "        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
        "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
        "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
        "            a batch has varying length sentences.\n",
        "        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size]\n",
        "            with indices selected in [0, ..., num_labels].\n",
        "    Outputs:\n",
        "        if `labels` is not `None`:\n",
        "            Outputs the CrossEntropy classification loss of the output with the labels.\n",
        "        if `labels` is `None`:\n",
        "            Outputs the classification logits of shape [batch_size, num_labels].\n",
        "    Example usage:\n",
        "    ```python\n",
        "    # Already been converted into WordPiece token ids\n",
        "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
        "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
        "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
        "    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
        "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
        "    num_labels = 2\n",
        "    model = BertForSequenceClassification(config, num_labels)\n",
        "    logits = model(input_ids, token_type_ids, input_mask)\n",
        "    ```\n",
        "    \"\"\"\n",
        "    def __init__(self, config, num_labels=2):\n",
        "        super(BertForSequenceClassification, self).__init__()\n",
        "        self.num_labels = num_labels\n",
        "        self.config = config\n",
        "        #self.bert = BertModel(config=config) # Uncomment if you want to do castom config\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
        "        nn.init.xavier_normal_(self.classifier.weight)\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
        "        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        return logits\n",
        "    def freeze_bert_encoder(self):\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = False\n",
        "    \n",
        "    def unfreeze_bert_encoder(self):\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oa-lw9IVEq3a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
        "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
        "\n",
        "num_labels = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xX6evit25eVu",
        "colab_type": "text"
      },
      "source": [
        "Defining BERT model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNG4bwXXgtor",
        "colab_type": "code",
        "outputId": "e0d42936-2560-437f-e40e-438a71a2d7a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        }
      },
      "source": [
        "model = BertForSequenceClassification(config, num_labels)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:pytorch_pretrained_bert.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz not found in cache, downloading to /tmp/tmprzis412_\n",
            "100%|██████████| 407873900/407873900 [00:05<00:00, 78428599.98B/s]\n",
            "INFO:pytorch_pretrained_bert.file_utils:copying /tmp/tmprzis412_ to cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
            "INFO:pytorch_pretrained_bert.file_utils:creating metadata file for /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
            "INFO:pytorch_pretrained_bert.file_utils:removing temp file /tmp/tmprzis412_\n",
            "INFO:pytorch_pretrained_bert.modeling:loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
            "INFO:pytorch_pretrained_bert.modeling:extracting archive file /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmp9lyvgo7u\n",
            "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVs2vsda5rWp",
        "colab_type": "text"
      },
      "source": [
        "Prepearing data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_u2XuqREEq3g",
        "colab_type": "code",
        "outputId": "7e312516-952c-4ee6-ad86-9f3a74daa4c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_json(\"Sarcasm_Headlines_Dataset_v2.json\", lines=True)\n",
        "df.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>is_sarcastic</th>\n",
              "      <th>headline</th>\n",
              "      <th>article_link</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>thirtysomething scientists unveil doomsday clo...</td>\n",
              "      <td>https://www.theonion.com/thirtysomething-scien...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>dem rep. totally nails why congress is falling...</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/donna-edw...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>eat your veggies: 9 deliciously different recipes</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/eat-your-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>inclement weather prevents liar from getting t...</td>\n",
              "      <td>https://local.theonion.com/inclement-weather-p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>mother comes pretty close to using word 'strea...</td>\n",
              "      <td>https://www.theonion.com/mother-comes-pretty-c...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   is_sarcastic  ...                                       article_link\n",
              "0             1  ...  https://www.theonion.com/thirtysomething-scien...\n",
              "1             0  ...  https://www.huffingtonpost.com/entry/donna-edw...\n",
              "2             0  ...  https://www.huffingtonpost.com/entry/eat-your-...\n",
              "3             1  ...  https://local.theonion.com/inclement-weather-p...\n",
              "4             1  ...  https://www.theonion.com/mother-comes-pretty-c...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94YCTwJLH1RS",
        "colab_type": "code",
        "outputId": "beabf190-be72-4413-c4da-d985e5e3f66b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "df = df.drop(['article_link'], axis=1)\n",
        "df.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>is_sarcastic</th>\n",
              "      <th>headline</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>thirtysomething scientists unveil doomsday clo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>dem rep. totally nails why congress is falling...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>eat your veggies: 9 deliciously different recipes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>inclement weather prevents liar from getting t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>mother comes pretty close to using word 'strea...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   is_sarcastic                                           headline\n",
              "0             1  thirtysomething scientists unveil doomsday clo...\n",
              "1             0  dem rep. totally nails why congress is falling...\n",
              "2             0  eat your veggies: 9 deliciously different recipes\n",
              "3             1  inclement weather prevents liar from getting t...\n",
              "4             1  mother comes pretty close to using word 'strea..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvSxEUK2Eq4H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = df['headline']\n",
        "y = df['is_sarcastic']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOX0oLliEq4O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.tolist()\n",
        "X_test = X_test.tolist()\n",
        "\n",
        "y_train = y_train.tolist()\n",
        "y_test = y_test.tolist()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PypbNzeS5282",
        "colab_type": "text"
      },
      "source": [
        "As most headlines have length less then 24, we will define max_seq_length = 24:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBCLwuUjEq4T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_seq_length = 24\n",
        "class text_dataset(Dataset):\n",
        "    def __init__(self,x_y_list, transform=None):\n",
        "        \n",
        "        self.x_y_list = x_y_list\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __getitem__(self,index):\n",
        "        \n",
        "        tokenized_review = tokenizer.tokenize(self.x_y_list[0][index])\n",
        "        \n",
        "        if len(tokenized_review) > max_seq_length:\n",
        "            tokenized_review = tokenized_review[:max_seq_length]\n",
        "            \n",
        "        ids_review  = tokenizer.convert_tokens_to_ids(tokenized_review)\n",
        "\n",
        "        padding = [0] * (max_seq_length - len(ids_review))\n",
        "        \n",
        "        ids_review += padding\n",
        "        \n",
        "        assert len(ids_review) == max_seq_length\n",
        "        \n",
        "        #print(ids_review)\n",
        "        ids_review = torch.tensor(ids_review)\n",
        "        \n",
        "        sentiment = self.x_y_list[1][index] # color        \n",
        "        list_of_labels = [torch.from_numpy(np.array(sentiment))]\n",
        "        \n",
        "        \n",
        "        return ids_review, list_of_labels[0]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.x_y_list[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3-3W5cSEq4V",
        "colab_type": "code",
        "outputId": "e4298284-63a1-4846-827c-0399bc0f87cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_lists = [X_train, y_train]\n",
        "test_lists = [X_test, y_test]\n",
        "\n",
        "training_dataset = text_dataset(x_y_list = train_lists )\n",
        "\n",
        "test_dataset = text_dataset(x_y_list = test_lists )\n",
        "\n",
        "dataloaders_dict = {'train': torch.utils.data.DataLoader(training_dataset, batch_size=batch_size, shuffle=True, num_workers=0),\n",
        "                   'val':torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "                   }\n",
        "dataset_sizes = {'train':len(train_lists[0]),\n",
        "                'val':len(test_lists[0])}\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vteX76PFEq4Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "    print('starting')\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss = 100\n",
        "\n",
        "    val_loss = []\n",
        "    train_loss = []\n",
        "    val_acc = []\n",
        "    train_acc = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0          \n",
        "            sentiment_corrects = 0\n",
        "            tp = 0.0 #true poisitve\n",
        "            tn = 0.0 #true negative\n",
        "            fp = 0.0 #false positive\n",
        "            fn = 0.0 #false negative\n",
        "            \n",
        "            # Iterate over data.\n",
        "            for inputs, sentiment in dataloaders_dict[phase]:\n",
        "\n",
        "                inputs = inputs.to(device) \n",
        "                sentiment = sentiment.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "\n",
        "                    outputs = model(inputs)\n",
        "                    outputs = F.softmax(outputs,dim=1)\n",
        "\n",
        "                    loss = criterion(outputs, sentiment)\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        \n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "                sentiment_corrects += torch.sum(torch.max(outputs, 1)[1] == sentiment)\n",
        "\n",
        "                tp += torch.sum(torch.max(outputs, 1)[1] & sentiment)\n",
        "                tn += torch.sum(1-torch.max(outputs, 1)[1] & 1-sentiment)\n",
        "                fp += torch.sum(torch.max(outputs, 1)[1] & 1-sentiment)\n",
        "                fn += torch.sum(1-torch.max(outputs, 1)[1] & sentiment)\n",
        "\n",
        "                \n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "\n",
        "            sentiment_acc = sentiment_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            if phase == 'train':\n",
        "                train_acc.append(sentiment_acc)\n",
        "                train_loss.append(epoch_loss)\n",
        "            elif phase == 'val':\n",
        "                val_acc.append(sentiment_acc)\n",
        "                val_loss.append(epoch_loss)\n",
        "\n",
        "            print('{} total loss: {:.4f} '.format(phase,epoch_loss ))\n",
        "            print('{} sentiment_acc: {:.4f}'.format(\n",
        "                phase, sentiment_acc))\n",
        "\n",
        "            if phase == 'val' and epoch_loss < best_loss:\n",
        "                print('saving with loss of {}'.format(epoch_loss),\n",
        "                      'improved over previous {}'.format(best_loss))\n",
        "                best_loss = epoch_loss\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                torch.save(model.state_dict(), 'bert_model_test.pth')\n",
        "\n",
        "            if phase == 'val' and epoch == num_epochs - 1:\n",
        "                recall = tp / (tp + fn)\n",
        "                print('recall {:.2f}'.format(recall))\n",
        "\n",
        "        print()\n",
        "    \n",
        "    confusion_matrix = [[int(tp), int(fp)],[int(fn), int(tn)]]\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val loss: {:4f}'.format(float(best_loss)))\n",
        "\n",
        "    results = {'time': time_elapsed, 'recall': recall, 'conf_matr': confusion_matrix, 'val_loss': val_loss, 'train_loss': train_loss, 'val_acc': val_acc, 'train_acc': train_acc} \n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Bpdj3zFEq4b",
        "colab_type": "code",
        "outputId": "f9980255-f5de-49ac-e606-690d135a00dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.to(device)\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BQYohJw7V8Y",
        "colab_type": "text"
      },
      "source": [
        "We will use two learning rates: 1st is for classifier, so it can be more aggressive and 2nd one for pre-trained bert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpirB1t1Eq4d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lrlast = .001\n",
        "lrmain = .00001\n",
        "optimizer = optim.Adam(\n",
        "    [\n",
        "        {\"params\":model.bert.parameters(),\"lr\": lrmain},\n",
        "        {\"params\":model.classifier.parameters(), \"lr\": lrlast},\n",
        "       \n",
        "   ])\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 4 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "rDOkYH2_Eq4h",
        "colab_type": "code",
        "outputId": "c4d9daea-757f-4e08-91a8-8c7d5ccac8e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model_fit, res = train_model(model, criterion, optimizer, exp_lr_scheduler,\n",
        "                       num_epochs=8)\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "starting\n",
            "Epoch 1/8\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train total loss: 0.4906 \n",
            "train sentiment_acc: 0.8114\n",
            "val total loss: 0.4381 \n",
            "val sentiment_acc: 0.8709\n",
            "saving with loss of 0.43814353977026094 improved over previous 100\n",
            "\n",
            "Epoch 2/8\n",
            "----------\n",
            "train total loss: 0.4288 \n",
            "train sentiment_acc: 0.8796\n",
            "val total loss: 0.4313 \n",
            "val sentiment_acc: 0.8767\n",
            "saving with loss of 0.4312747877265589 improved over previous 0.43814353977026094\n",
            "\n",
            "Epoch 3/8\n",
            "----------\n",
            "train total loss: 0.4031 \n",
            "train sentiment_acc: 0.9068\n",
            "val total loss: 0.4096 \n",
            "val sentiment_acc: 0.9006\n",
            "saving with loss of 0.40960756852275254 improved over previous 0.4312747877265589\n",
            "\n",
            "Epoch 4/8\n",
            "----------\n",
            "train total loss: 0.3808 \n",
            "train sentiment_acc: 0.9305\n",
            "val total loss: 0.4102 \n",
            "val sentiment_acc: 0.8983\n",
            "\n",
            "Epoch 5/8\n",
            "----------\n",
            "train total loss: 0.3762 \n",
            "train sentiment_acc: 0.9352\n",
            "val total loss: 0.4048 \n",
            "val sentiment_acc: 0.9055\n",
            "saving with loss of 0.404751583152847 improved over previous 0.40960756852275254\n",
            "\n",
            "Epoch 6/8\n",
            "----------\n",
            "train total loss: 0.3751 \n",
            "train sentiment_acc: 0.9360\n",
            "val total loss: 0.4042 \n",
            "val sentiment_acc: 0.9067\n",
            "saving with loss of 0.40422910320683847 improved over previous 0.404751583152847\n",
            "\n",
            "Epoch 7/8\n",
            "----------\n",
            "train total loss: 0.3712 \n",
            "train sentiment_acc: 0.9399\n",
            "val total loss: 0.4013 \n",
            "val sentiment_acc: 0.9095\n",
            "saving with loss of 0.40128467157535164 improved over previous 0.40422910320683847\n",
            "\n",
            "Epoch 8/8\n",
            "----------\n",
            "train total loss: 0.3688 \n",
            "train sentiment_acc: 0.9428\n",
            "val total loss: 0.4026 \n",
            "val sentiment_acc: 0.9090\n",
            "recall 0.89\n",
            "\n",
            "Training complete in 12m 15s\n",
            "Best val loss: 0.401285\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezNKXtnk87kA",
        "colab_type": "text"
      },
      "source": [
        "Lets see at test and train accuracy plots:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8THlZmAt8uab",
        "colab_type": "code",
        "outputId": "3bda4947-fecd-466a-f836-74b268bea4ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "plt.plot(res['train_acc'], label = 'Train Accuracy')\n",
        "plt.plot(res['val_acc'], label = 'Test Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f9a4c9339b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEGCAYAAABrQF4qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3deXhU5fn/8fedfQ9kISyBBDGiKMgS\nNrFaURQVxbWgIIsLbrRaa/3qT+tea7+ttYvaFhUElyDKV8W6UKm4lTXsqwoSQgiQkAgkZE/u3x9n\nCJMQyABJJjO5X9eVa86cbe4J4TNnnvOc54iqYowxxn8FeLsAY4wxzcuC3hhj/JwFvTHG+DkLemOM\n8XMW9MYY4+eCvF1AfQkJCZqamurtMowxxqesWLFir6omNrSs1QV9amoqmZmZ3i7DGGN8iohsP9oy\na7oxxhg/Z0FvjDF+zoLeGGP8XKtro29IZWUlOTk5lJWVebsUcxzCwsJITk4mODjY26UY06b5RNDn\n5OQQHR1NamoqIuLtcowHVJWCggJycnLo3r27t8sxpk3ziaabsrIy4uPjLeR9iIgQHx9v38KMaQV8\nIugBC3kfZP9mxrQOPtF0Y4wx/kRVOVBaRV5RGXlF5ew54DzGhAVz4+BuTf56FvQeKCgo4MILLwRg\n9+7dBAYGkpjoXIC2bNkyQkJCGt3H5MmTefDBB+nZs+dxvfaoUaPYt28f33zzzfEXboxpUTU1yo8l\nFew5UF4b4vlF5eQdqBvo+UXllFfVHLF9v27tvBf0IjIS+AsQCLyiqs/WW54CTAcSgUJgvKrmuC2P\nATYC76vq1CaqvcXEx8ezevVqAB5//HGioqK4//7766yjqqgqAQENt4bNmDHjuF+3sLCQtWvXEhYW\nRnZ2Nt26Nf0fAEBVVRVBQfaZb8zRVFXXUHCwgjy3AD8U2nkHysl3C/WqmiNv5hQTFkSHmDA6RIeS\nntK+drr20TUdFdo8/w8b3auIBAIvAiOAHGC5iMxT1Y1uq/0RmKWqM0VkOPA74Ca35U8BXzVd2a3D\nli1buPLKK+nXrx+rVq3is88+44knnmDlypWUlpYyZswYHn30UQDOPfdcXnjhBc466ywSEhK44447\n+OSTT4iIiOCDDz6gQ4cOR+z/3Xff5aqrriI2NpbZs2fzwAMPAM63ittvv51t27YhIkybNo3Bgwcz\nY8YMnn/+eUSE/v37M2PGDMaPH891113HVVddBUBUVBTFxcUsWLCAp59+mqioKLZu3cqmTZu44oor\nyM3NpaysjF/+8pfceuutAHz00Uf85je/obq6mqSkJD799FNOO+00li1bRlxcHNXV1aSlpZGZmUlc\nXFwL/faNOXkVVTXkFztH3HvcArtuoJdTeLCcBvKbuMiQ2pBOS4quE9rOdBgdYkIJCw5s+TfnxpOP\nj0HAFlX9AUBEZgOjcY7QD+kF3OeaXgi8f2iBiAwAkoBPgfSTLfiJDzewMffAye6mjl6dY3jsijNP\naNvNmzcza9Ys0tOdt/bss88SFxdHVVUVF1xwAddddx29evWqs83+/fs5//zzefbZZ7nvvvuYPn06\nDz744BH7zsjI4JlnniE2NpZx48bVBv3dd9/NiBEjmDp1KlVVVZSUlLBmzRp+//vfs2jRIuLi4igs\nLGy09szMTDZu3Fj7TWHmzJnExcVRUlJCeno61157LeXl5dx55518/fXXpKSkUFhYSEBAADfccANv\nvfUWU6dOZf78+QwcONBC3rQaqkp+UTnZhSV1mlFqA9z1+GNJ5RHbBggkRIXSISaUpJgweneJpUN0\nKIkxYSS5hXhCVCghQb7Rn8WToO8C7HB7ngMMrrfOGuAanOadq4FoEYkHfgSeA8YDFx3tBURkCjAF\naLbmiebSo0eP2pAHJ5xfffVVqqqqyM3NZePGjUcEfXh4OJdeeikAAwYM4Ouvvz5iv7m5uWRnZzN0\n6FAAampq2Lx5M6effjpffPEFs2fPBiAoKIiYmBg+//xzxowZUxu2noTu0KFD6/y+n3/+eebNmwc4\n1y5s3bqVHTt2cMEFF5CSklJnv7fccgvXX389U6dOZfr06bVH/8a0lKrqGnbuK2V7QQnbC0vILjjo\nTBeUkF1YQmlldZ31gwOFxCgnsFPiIxjYvb1zxB3thPqh6fioUAID/KvHWFM1CN0PvCAik3CaaHYC\n1cBdwMeqmnOsrnaqOg2YBpCenn7Mu5Wf6JF3c4mMjKyd/v777/nLX/7CsmXLaNeuHePHj2+wH7n7\nydvAwECqqqqOWOftt99m7969HBqyef/+/WRkZPDEE08AnnddDAoKoqbGOelTXV1d57Xca1+wYAFf\nffUVS5YsITw8nHPPPfeYfeBTU1Np3749CxcuZNWqVVx88cUe1WPM8SitqCa7sITth0K88GBtkO/8\nsbROe3hoUADd4iJIiY/k3LQEUuIj6BoXQafYMDpEh9EuPJgAPwtwT3kS9DuBrm7Pk13zaqlqLs4R\nPSISBVyrqvtEZCjwExG5C4gCQkSkWFWPbKfwAwcOHCA6OpqYmBh27drF/PnzGTly5AntKyMjgwUL\nFjBw4EDA+RC5/PLLeeKJJ7jgggv4xz/+wdSpU6murubgwYMMHz6cMWPGcM8999Q23cTFxZGamsqK\nFSu45ppreO+996iurm7w9fbv309cXBzh4eFs2LCB5cuXA3DOOedwzz33sH379tqmG/ej+nHjxjF5\n8uSjnoQ2pjH7SirIKnDCPLv26LyErIKD5BWV11k3JiyI1IRIeneJZVSfTqTERZIS74R7h+jQNhvk\njfEk6JcDaSLSHSfgxwI3uq8gIglAoarWAA/h9MBBVce5rTMJSPfXkAfo378/vXr14vTTTyclJYVh\nw4ad0H62bt3Krl276jQJpaWlERYWxooVK3jhhRe47bbb+Oc//0lQUBD//Oc/GTRoEA888ADnnXce\nQUFBDBgwgFdffZXbb7+d0aNH869//YtRo0YRGhra4GtefvnlTJs2jV69etGzZ08GD3Za55KSkvj7\n3//O6NGjUVU6d+7MJ598AsDVV1/NzTffzKRJk07ofZq2oaZG2VNU5hyJuwL8UJhvLzjIgbK632iT\nYkJJiYvk/NMSSYmPoFt8JClxEaTER9AuovGuzOZIonrMlhJnJZHLgD/jdK+crqq/FZEngUxVnSci\n1+H0tFGcppu7VbW83j4m4QT9MbtXpqena/0bj2zatIkzzjjD83dlWsSSJUt46KGHWLhw4VHXsX+7\ntqGiymkvzzp0VF5QQnbhQbIKSthRWFKnz3hQgNClfTgpbgHeLS6C1IRIuraPIDzEuz1UfJWIrFDV\nBju8eNRGr6ofAx/Xm/eo2/S7wLuN7OM14DVPXs+0fr/97W+ZNm1a7Ulh4/tUlcpqpaK6hooqt5/q\naiqqDs8vPFjO9oISslxhvr2ghNx9pXW6H4YHB5ISH8EpCZFc0DPRCfX4CFLiIuncLoygQGvqa0l2\nlYw5IQ8//DAPP/ywt8vwC8XlVZRUVLkFa92gLT8ieOtOlx8RynXXKz/KdvVfo7K6Bg++4NdqHxFM\nt/hIBqS055p+XZwmlvgIUuIiSIwOtbGOWhELemNawKF+3VvyitmSX8z3e4rZklfM93nF7C0ub3wH\njQgJDCAkyPVzlOmo0CBCIuouC3Y9hja0rdvzw8sDaRcRTLf4CGLC7D4DvsKC3pgmVFOj7NxX6gR6\n3qEwL2JLXnGdk47RoUGcmhTFBT0TOSUxiuiwoMOB6kFo15+2o2dzLBb0xpyAyuoatheUsCWvmK35\nxXy/p4gt+cVszTtY50KdhKgQeiRGcWXfzpyaGEVaUjSndoiigzVtmBZkQW/MMZRVVvND/kG25Bez\nxRXm3+8pJqvgIJXVhxu0O8eGcWpSNIMGxZOWFMWpHaI4NTGK9pHWHdB4nwW9B5pimGKA6dOnc9ll\nl9GxY8cGl1dUVNCxY0fuuusunn766aYp3nikqKzycHNLfjFb9jiP2YUltScoAwRS4iPpkRjFhWck\nkdbBCfQeHaKabdRBY5qC/XV6wJNhij0xffp0+vfvf9Sgnz9/Pr169eLtt99u1qBvy8MSFxTXPSG6\n1fW4+8Dh4R5CAgPonhDJWZ1juapvF07tEEVaUhSp8ZFeH4XQmBPRNv+3N6GZM2fy4osvUlFRwTnn\nnMMLL7xATU0NkydPZvXq1agqU6ZMISkpidWrVzNmzBjCw8Mb/CaQkZHBfffdx/PPP8+yZcsYNGgQ\nAEuXLuXee++lpKSEsLAwFi5cSEhICL/+9a/57LPPCAgI4I477uCuu+4iOTmZ9evX065dO5YsWcIj\njzzCggULeOSRR8jOzmbr1q10796dJ554gkmTJlFcXExAQAAvvfRS7dWwzzzzDBkZGQQEBDBq1Cgm\nTJjA+PHja4dF2LRpExMnTmTZsmUt+8v2kKqy+0CZcyJ0T90j9MKDFbXrRYQE0iMxinN6xNOjQ1Tt\nEXq3uAjr5238iu8F/ScPwu51TbvPjr3h0mcbX6+e9evX895777Fo0SKCgoKYMmUKs2fPpkePHuzd\nu5d165w69+3bR7t27fjb3/7GCy+8QN++fY/YV0lJCV988QXTp09n9+7dZGRkMGjQIMrKyhg7dixz\n586lf//+7N+/n9DQUF566SVyc3NZs2YNgYGBHg1LvHnzZr766ivCwsIoKSnhs88+IywsjM2bNzNx\n4kSWLl3Khx9+yCeffMKyZcsIDw+vHdsmPDyc9evXc9ZZZzFjxgwmT5583L+vlvDjwQp+nrGKb7bs\nrZ0XGx5MWocoLu6V5LSdd3BOinaKCbOxUUyb4HtB34osWLCA5cuX145JU1paSteuXbnkkkv49ttv\n+cUvfsHll1/u0ciO8+bNY8SIEYSFhXH99dczYMAAnnvuOTZt2kS3bt3o378/ALGxsbWvfe+99xIY\n6DQleDIs8ejRowkLCwOgvLycqVOnsmbNGoKCgti6dWvtfm+++WbCw8Pr7PeWW25hxowZ/P73v+ed\nd95h1apVx/OrahHf7i7i1lnL2bO/nF9f0pP+3dpzaocoEqJCrIeLadN8L+hP4Mi7uagqN998M089\n9dQRy9auXcsnn3zCiy++yNy5c5k2bdox95WRkcGSJUtqhyXOz8/nyy+/pF27dsdVk/uwxPWHGXYf\nlvi5556ja9euvPHGG1RWVhIVFXXM/V5//fU888wzDBs2jKFDhx53Xc3ts417uHf2KiJCg5h9+xD6\nd2vv7ZKMaTWsIfIkXHTRRcyZM4e9e51mgoKCArKzs8nPz0dVuf7663nyySdZuXIlANHR0RQVFR2x\nn3379rFkyRJycnLIysoiKyuLv/71r2RkZNCrVy+ys7Nr93HgwAGqq6sZMWIE//jHP2qHHT7UdHNo\nWGKAuXPnHrX2/fv306lTJ0SEmTNncmhwuxEjRjB9+nRKS0vr7DciIoLhw4czderUVtVso6q8uHAL\nU17P5JTEKOZNHWYhb0w9FvQnoXfv3jz22GNcdNFF9OnTh4svvpg9e/awY8cOzjvvPPr27cvkyZN5\n5plnAJg8eTK33norffv2paLi8EnBuXPnMmLECIKDD19SftVVV/H+++8TEBBARkYGd955J2effTYX\nX3wx5eXl3H777XTs2JE+ffpw9tlnM2fOHMDpFXTXXXcxcODAY3b7nDp1Kq+88gpnn30227Ztqx2+\neNSoUYwcOZL09HT69u3L888/X7vNuHHjCA4Oru1q6m2lFdX8YvZq/jD/W67o05l37hhKp9hwb5dl\nTKvj0TDFLcmGKW69nn32WcrLy3nsscc83qa5/u127S9lyqwVrM/dz68v6cmd5/ewdnjTpp30MMXG\nXHHFFezYsYPPP//c26WwYvuP3P76Ckorqnj5pnQu6pXk7ZKMadUs6I1HPvzwQ2+XAMC7K3L4f/+3\njo6xYbx122BOS4r2dknGtHo+E/Sqal/NfUxTNgtW1yi/+3gTr3yzjaGnxPPSuP42jowxHvKJoA8L\nC6OgoID4+HgLex+hqhQUFNT22z8Z+0sr+UXGKr78Lp+JQ1N4ZFQvgu3KVWM85hNBn5ycTE5ODvn5\n+d4uxRyHsLAwkpOTT2ofW/OLuW1mJtmFJTxzdW9uHNytiaozpu3wiaAPDg6me/fu3i7DtLAvv8tn\n6lsrCQ4M4M1bBzP4lHhvl2SMT/KJoDdti6ry6jfbeObjTZyWFM3LE9LpGhfh7bKM8VkeNXSKyEgR\n+VZEtojIgw0sTxGR/4jIWhH5QkSSXfP7ishiEdngWjamqd+A8S9lldXc/85anv5oExf36sjcO8+x\nkDfmJDV6RC8igcCLwAggB1guIvNUdaPban8EZqnqTBEZDvwOuAkoASao6vci0hlYISLzVXVfk78T\n4/Pyisq4/fUVrMrexz0XpnHPhWk2uqQxTcCTpptBwBZV/QFARGYDowH3oO8F3OeaXgi8D6Cq3x1a\nQVVzRSQPSAQs6E0d63L2M+X1TPaVVPLSuP5c1ruTt0syxm940nTTBdjh9jzHNc/dGuAa1/TVQLSI\n1DlzJiKDgBBg64mVavzVvDW5XPePRQSI8O6dQy3kjWliTdUZ+X7gfBFZBZwP7ASqDy0UkU7A68Bk\nVa2pv7GITBGRTBHJtC6UbUdNjfKH+Zv5RcYqeneJ5YOpwzizc6y3yzLG73jSdLMT6Or2PNk1r5aq\n5uI6oheRKODaQ+3wIhIDfAQ8rKpLGnoBVZ0GTANnULPjfA/GBxWXV3Hv7NUs2LSHMeldeeqqswgJ\nsougjGkOngT9ciBNRLrjBPxY4Eb3FUQkASh0Ha0/BEx3zQ8B3sM5UftuUxZufFd2QQm3zlrO1vyD\nPH5FLyaek2pXPBvTjBoNelWtEpGpwHwgEJiuqhtE5EkgU1XnAT8FficiCnwF3O3a/GfAeUC8iExy\nzZukqqub9m0YX7Fo617uenMlqjBz8iDOTUvwdknG+D2fGI/e+IfXF2fx+Icb6Z4QycsT0umeENno\nNsYYz9h49MarKqpqeOLDDby5NJvhp3fgz2P7EhMW3PiGxpgmYUFvmlVBcTl3vbmSpdsKueP8Hvz6\nkp4E2kVQxrQoC3rTbDbtOsBtszLJKyrnz2P6clW/+pdfGGNaggW9aRafrt/NfXNWEx0WxDu3D+Xs\nru28XZIxbZYFvWlSqsrfPt/Cnz77jrO7tmPaTQNIijn5m48YY06cBb1pMiUVVfz6nbV8tG4XV/fr\nwu+u6U1YcKC3yzKmzbOgN01i575SpszKZOOuAzx06elMOe8UuwjKmFbCgt6ctMysQu54YwXllTVM\nnziQC07v4O2SjDFuLOjNSZmzfAcPv7+OLu3CmT0lnVM7RHu7JGNMPRb05oRUVdfw2483MeO/Wfwk\nLYEXbuhPbIRdBGW8rLIUinZD8R7nsWQvIBAQBIHBzmOd6WAIDDrKdDAEBLpNN7BtgG8MxGdBb47b\n/pJKpmas5Ovv9zJ5WCoPX3YGQYG+8QdvfJAqlO2Doj1QvPvwY3Fe3VAv3gPlB1q2Ngnw8EPiWB8Y\nbs/jT4WfHnG31pNmQW+Oy5a8Im6dmcnOfaX8/trejBnYzdslGV9VXQUH852Adg/rQ4/Fe1yhvgeq\ny4/cPigcopMgqiMk9YIeF0BUEkR3dOZFdYDIRBCB6kqoqXJ+aqcrnRoOTddUuZ43NF1/+0qoqfZs\nv9Wudd33W1UGFcVHbl9Z2iy/agt647GFm/P4RcYqQoMDyLhtCOmpcd4uybRG7s0ntWHtfiTumley\nF468DxGEtz8c1ClD3cI7qe50aLQT4qZRFvSmUarKtK9+4NlPN3NGxxhenphOl3bh3i7LtISqCqg8\nCBUlToBXHoTSHw8faR9xJJ4H5fuP3I8EOsEdlQQxXaBzPyfMDx2RR3c8vDwotOXfp5+zoDfHVFOj\nPPXRRmb8N4vLe3fiD9f3ISLE/mxajZpqqCxxBXGJ2/TBY8+rOOgK7kPTbutUlh6erqk69usHRxw+\n0k7qBT2Gu8I7qW6QR8Q57dTGK+x/rDmq6hrlof9by5zMHCYPS+U3l/ciwEaebDqqsG875GQ6R8lH\nC9tjhXNV2fG9pgRAcCQEh0NIhDMdEuEEdkS88xgcDiGRznT9dYIjICzWmk98jAW9aVBFVQ2/fHs1\nH63bxS8uTOOXF6XZla4nSxV+3AZZ30DWf53HAzlHrhcU5grZemEbmXhk8Ia4QrvO+u7bRdSdFxRq\nwdwGWdCbI5RWVHPnmyv44tt8Hr7sDG477xRvl+SbVKHwB8j6+nCwF+U6yyISIHUYpNxz+ITjoaNp\na+IwTcyC3tRRVFbJLa9lsnx7Ib+7pjc3DLLukx5ThYItdYO9eLezLLKDK9iHQepPILGnHVmbFmNB\nb2oVHqxg4vRlbNp1gL+M7ceVZ3f2dkmtmyrs/e5wsG//r9P7BJwTkKnDIPVcSDkXEtIs2I3XWNAb\nAPYcKGP8K0vJLixh2oQBDD89ydsltT6qkL/Z1cb+jRPsB/OdZdGdoft5h4M9vocFu2k1PAp6ERkJ\n/AUIBF5R1WfrLU8BpgOJQCEwXlVzXMsmAo+4Vn1aVWc2Ue2miWQXlDDu1SUUFlfw2uRBDO0R7+2S\nWoeaGsjfVDfYSwqcZTHJTlfC1HOd5pi4UyzYTavVaNCLSCDwIjACyAGWi8g8Vd3ottofgVmqOlNE\nhgO/A24SkTjgMSAdUGCFa9sfm/qNmBPz/Z4ixr+6lLLKGt68bQh9j3bLP1X/D7KaGsjbUDfYS11/\nqrHdIO2Sw80x7VL8//dh/IYnR/SDgC2q+gOAiMwGRgPuQd8LuM81vRB43zV9CfCZqha6tv0MGAlk\nnHzp5mSty9nPhOlLCQoMYM7tQ+nZ8ShDDG/+CD642xmjIyLO6W/t/hMZf+S8iHjnUvbW3IOkphp2\nr3MC/VAbe9k+Z1m7FOh5+eETqO1TvFurMSfBk6DvAuxwe54DDK63zhrgGpzmnauBaBGJP8q2XU64\nWtNklv5QwC0zM4kND+bNWweTmhB55Eqq8PVz8PlT0KkvpJzjNF2UFDht0/nfOtOVB4/yKuKEfZ0P\nALcPisiEI+eHxjTfkXJ1Fexe6wr2b2D74sOX67fvDmdccbgppl3X5qnBGC9oqpOx9wMviMgk4Ctg\nJ1Dt6cYiMgWYAtCtm3Xna25ffJvH7a+vILl9OG/cOphOsQ2MW1NZCvN+DuvegbOug9EvOH28G1JZ\nCiWFhz8EjvazbzvkroSDe52R/BoSENTwt4P6HxbuHxJHq6u6Cnatge2uppjsJYeHsY3rAWdedTjY\nY+34w/gvT4J+J+B+eJPsmldLVXNxjugRkSjgWlXdJyI7gZ/W2/aL+i+gqtOAaQDp6enqefnmeH28\nbhf3zF5FWodoZt0yiISoBgaQKtoNs2+EnStg+G/gJ7869lF2cLgTlJ6GpSqUF7k+ABr5gMjbeHg9\njvKncejyffdvC6U/QvZSqChy1kk4Dc669nCwx3TyrFZj/IAnQb8cSBOR7jgBPxa40X0FEUkAClW1\nBngIpwcOwHzgGRFp73p+sWu58YI5mTt4cO5a+nVrz/RJA4kNb+COULmrIONGKNsPY95wmjOamgiE\nxTg/cd0926am2qmptulob70PBbcPjMJtzjACfX52ONijrbuoabsaDXpVrRKRqTihHQhMV9UNIvIk\nkKmq83CO2n8nIorTdHO3a9tCEXkK58MC4MlDJ2ZNy5r+zTae/NdGfpKWwD9vGtDwCJTr/w/ev8tp\nFrllPnTs3fKFHk1AoOuIPQ5I83Y1xvgUUW1dLSXp6emamZnp7TL8hqry1/9s4fkF33HJmUn89YZ+\nhAbV6wlTUwNfPgtf/h66DnGO5KMSvVOwMeaEiMgKVU1vaJldGevHVJVnPt7Ey19v45r+Xfjfa/sc\neW/XioPw3h2waR70HQejnrcbPxjjZyzo/VR1jfLI++vIWLaDiUNTeOyKM48cS35/DmSMhd3r4eKn\nYehUuwjIGD9kQe+HKqudseT/tXYXd1/Qg/sv7nnkWPI7lsHscU7XyBvnwGkXe6dYY0yzs6D3M2WV\n1dz15ko+35zHg5eezh3n9zhypTWznT7yMZ1h4ofQ4fSWL9QY02Is6P1IUVklt87MZFlWIU9fdRbj\nh9S7bL+mGv7zJPz3z86Y6D+b5erFYozxZxb0fuLHgxVMmrGM9bkH+POYvozuW+/ipfIimHsbfPcJ\nDJgMl/0BAhvoR2+M8TsW9H4g70AZN726jG0FB/nH+AGM6FXv4qAfsyDjBmdsmsv+CANvtZOuxrQh\nFvQ+bkdhCeNfXUp+UTkzJg1k2KkJdVfI+i+8PR60GsbPhR4XeKdQY4zXWND7sC15xYx/ZSklFVW8\ncetg+ndrX3eFFTPho/uckRlvmA0Jp3qnUGOMV1nQ+6j1O/czYfoyAkR4+/ahnNEp5vDC6ir49yOw\n9O/OXZCumwHhR7mhiDHG71nQ+6DMrEImv7acmLBg3rh1MN3dx5Iv3QfvToatn8PgO50LoQLtn9mY\ntswSwMd89V0+t7++go6xYbxx62C6tHMbi33vFudK1x+3wRV/gQGTvFanMab1sKD3IZ+u38UvMlZz\nSmIkr98ymMRotzFpti6EdyaCBMKED5zheY0xBghofBXTGry7Ioe73lzJmV1ieHvK0MMhrwrLXoY3\nroXozjBloYW8MaYOO6L3ATMXZfHYvA0MOzWeaTelExnq+merroRPHoDM6XDaSLjmZedmHsYY48aC\nvhVTVV76Yit/mP8tI3ol8bcb+hEW7BpLvqQQ5kyArK9h2L1w4aPOzTmMMaYeC/pWSlV59tPN/PPL\nH7iqb2f+cP3ZBB8aSz5vM2SMgQO5cPU/4eyx3i3WGNOqWdC3QtU1ym8+WM9bS7MZN7gbT40+6/BY\n8t/9G9692bkh96SPoOsg7xZrjGn1LOhbmcrqGu5/Zw0frM7ljvN78D8jXWPJq8LiF+Dfv4GOZ8HY\nDGjX1dvlGmN8gAV9K1JWWc3Ut1ayYFMeD4zsyV0/dQ1ZUFUO//olrH4TzrgSrv4HhEQee2fGGONi\nQd9KHCyv4rZZmSzaWsBTo8/kpqGpzoLifHh7HOxYCuf/D5z/IARYr1hjjOcs6FuBfSUVTJqxnHU7\n9/Onn53NNf2TnQW71znDC2CJMWAAABUUSURBVB/Mh+umw1nXerdQY4xP8ujQUERGisi3IrJFRB5s\nYHk3EVkoIqtEZK2IXOaaHywiM0VknYhsEpGHmvoN+Lq8ojLGTlvCxtwDvHhj/8Mhv+lf8OolUFMF\nkz+xkDfGnLBGj+hFJBB4ERgB5ADLRWSeqm50W+0RYI6q/l1EegEfA6nA9UCoqvYWkQhgo4hkqGpW\nE78Pn5TzYwnjX1nKngPlvDopnZ+kJTonXb9+Dj5/Cjr3h7FvQUwnb5dqjPFhnjTdDAK2qOoPACIy\nGxgNuAe9AocuyYwFct3mR4pIEBAOVAAHmqBun7dt70HGvbyEovIq3rh1EANS4qCyFD6YCuvfhd7X\nw5V/c7pRGmPMSfAk6LsAO9ye5wCD663zOPBvEfk5EAlc5Jr/Ls6Hwi4gAvilqhbWfwERmQJMAejW\nrdtxlO+7Hv1gPSWV1cyeMoQzO8fCgV0w+0bIXelc5XrufXa7P2NMk2iq7hs3AK+pajJwGfC6iATg\nfBuoBjoD3YFficgp9TdW1Wmqmq6q6YmJiU1UUuu1Ja+Yr7/fy63ndndCfudKePkC556uY96En/zK\nQt4Y02Q8CfqdgPuVOcmuee5uAeYAqOpiIAxIAG4EPlXVSlXNA/4LpJ9s0b7u9cVZhAQGMHZQN1g/\nF2ZcCgFBcMt8OGOUt8szxvgZT4J+OZAmIt1FJAQYC8yrt042cCGAiJyBE/T5rvnDXfMjgSHA5qYp\n3TcVlVXy7oocruidRMKyPzrDGXTqC7cthI69vV2eMcYPNdpGr6pVIjIVmA8EAtNVdYOIPAlkquo8\n4FfAyyLyS5wTsJNUVUXkRWCGiGwABJihqmub7d20Zgf3Qu5qvl+6kOd0KcN37ITNudB3PIz6EwSF\nNr4PY4w5AaKq3q6hjvT0dM3MzPR2GSfHFersWuV6XAP7D5/Pzg3sTOfTh0Daxc7Ik9Yeb4w5SSKy\nQlUbbBq3K2NPVnE+7FrtCnTX44Gcw8vjejgjTA6awtqaVMZ/VMbjPzvn8IVRxhjTzCzoj0djoR5/\nKnQbAp37Ou3unfpAWGzt4r/OXE5w5D4u72MXQBljWo4F/dEU59UN9F2r4YBbZ6NGQr2+HYUl/Gdz\nHnf/9FRCg+xOUMaYlmNBDx6EehqknOMEeue+0LHPcd+b9fUl2wkQYdyQtnFBmDGm9Wh7QV+058jm\nl6JDIzaIc6R+kqFeX2lFNW8v38HIMzvSKdaGNDDGtCz/DvrGQj0hDVLPrdv8Ehrd5GV8sHon+0sr\nmTA0pcn3bYwxjfGfoK8shW1fQe6qw8FetMu10D3U+7mO1Hs3S6jXp6q8tiiL0ztGM6h7XLO/njHG\n1Oc/QV9eBG/9DCfUT4Pu57k1v7RMqDdkedaPbN5dxO+u6e3c+9UYY1qY/wR9VAe45TPo0AtCo7xd\nTa2Zi7KICQviqr5dvF2KMaaN8q+bj3Yd1KpCfvf+Mj7dsJsxA7sSHmJdKo0x3uFfQd/KvLl0OzWq\n3DQk1dulGGPaMAv6ZlJeVU3GsmyG9+xAt/gIb5djjGnDLOibycfrdrG3uIKJ56R6uxRjTBtnQd9M\nZi7azikJkZx7aoK3SzHGtHEW9M1gzY59rN6xjwlDUwgIsC6VxhjvsqBvBjMXZxEZEsi1A2woYmOM\n91nQN7G9xeX8a80urh2QTHRYsLfLMcYYC/qmNntZNhXVNUwYmurtUowxBrCgb1JV1TW8sSSbc09N\n4NQOrefCLWNM22ZB34T+vXEPuw+UWZdKY0yrYkHfhGYuyiK5fTjDT+/g7VKMMaaWR0EvIiNF5FsR\n2SIiDzawvJuILBSRVSKyVkQuc1vWR0QWi8gGEVknImFN+QZai027DrB0WyE3DUkh0LpUGmNakUZH\nrxSRQOBFYASQAywXkXmqutFttUeAOar6dxHpBXwMpIpIEPAGcJOqrhGReKCyyd9FKzBr8XZCgwL4\nWXpXb5dijDF1eHJEPwjYoqo/qGoFMBsYXW8dBQ7dby8WOHQbp4uBtaq6BkBVC1S1+uTLbl32l1Ty\n/qqdXNW3C+0jQ7xdjjHG1OFJ0HcBdrg9z3HNc/c4MF5EcnCO5n/umn8aoCIyX0RWisgDDb2AiEwR\nkUwRyczPzz+uN9AavLNiB6WV1Uw4x24VaIxpfZrqZOwNwGuqmgxcBrwuIgE4TUPnAuNcj1eLyIX1\nN1bVaaqarqrpiYmJTVRSy6iuUWYt3s7A1Pac2TnW2+UYY8wRPAn6nYB7w3Oya567W4A5AKq6GAgD\nEnCO/r9S1b2qWoJztN//ZItuTb78Lo/swhLrUmmMabU8CfrlQJqIdBeREGAsMK/eOtnAhQAicgZO\n0OcD84HeIhLhOjF7PrARP/Laou0kxYRyyZkdvV2KMcY0qNGgV9UqYCpOaG/C6V2zQUSeFJErXav9\nCrhNRNYAGcAkdfwI/Annw2I1sFJVP2qON+INW/OL+eq7fMYNTiE40C5JMMa0Th7dHFxVP8ZpdnGf\n96jb9EZg2FG2fQOni6XfeX3xdoIDhbGDrEulMab1ssPQE1RcXsW7K3K4vHcnOkT75TVgxhg/YUF/\ngt5bmUNxeRUT7CSsMaaVs6A/AarKzMXb6ZMcS7+u7bxdjjHGHJMF/QlYtLWALXnFTBiaioiNa2OM\nad0s6E/Aa4uyiIsMYVSfTt4uxRhjGmVBf5x2FJbwn017GDuwK2HBgd4uxxhjGmVBf5zeWLodgPFD\nbFwbY4xvsKA/DmWV1by9fAcX9+pI53bh3i7HGGM8YkF/HOatzmVfSaWNa2OM8SkW9B5SVV5blEXP\npGiGnBLn7XKMMcZjFvQeWrH9RzbuOsCEc1KsS6UxxqdY0HvotUVZRIcFcXW/+vdcMcaY1s2C3gN7\nDpTx6frd/Cy9KxEhHo0DZ4wxrYYFvQfeXJpNtSo3WZdKY4wPsqBvREVVDW8tzeanpyWSmhDp7XKM\nMea4WdA34pP1u9hbXG5dKo0xPsuCvhEzF2XRPSGS89J866blxhhziAX9MazL2c/K7H3cNCSFgADr\nUmmM8U0W9Mcwc3EWESGBXJee7O1SjDHmhFnQH0VBcTnz1uRyTf8uxIQFe7scY4w5YRb0RzF7+Q4q\nqmqYODTV26UYY8xJ8SjoRWSkiHwrIltE5MEGlncTkYUiskpE1orIZQ0sLxaR+5uq8OZUVV3Dm0u2\nc06PeNKSor1djjHGnJRGg15EAoEXgUuBXsANItKr3mqPAHNUtR8wFnip3vI/AZ+cfLktY8GmPeTu\nL7MulcYYv+DJEf0gYIuq/qCqFcBsYHS9dRSIcU3HArmHFojIVcA2YMPJl9syZi7aTpd24Vx4egdv\nl2KMMSfNk6DvAuxwe57jmufucWC8iOQAHwM/BxCRKOB/gCeO9QIiMkVEMkUkMz8/38PSm8e3u4tY\n/EMB44ekEBRopzCMMb6vqZLsBuA1VU0GLgNeF5EAnA+A51W1+Fgbq+o0VU1X1fTERO9emDRrcRYh\nQQGMGdjVq3UYY0xT8WQoxp2Ae+olu+a5uwUYCaCqi0UkDEgABgPXicj/Au2AGhEpU9UXTrryZrC/\ntJL/W7mT0Wd3Ji4yxNvlGGNMk/Ak6JcDaSLSHSfgxwI31lsnG7gQeE1EzgDCgHxV/cmhFUTkcaC4\ntYY8wLsrciitrLaTsMYYv9Jo042qVgFTgfnAJpzeNRtE5EkRudK12q+A20RkDZABTFJVba6im0NN\njfL64iwGpLTnrC6x3i7HGGOajEd30VDVj3FOsrrPe9RteiMwrJF9PH4C9bWYL7/PJ6ughF+OOM3b\npRhjTJOybiUuMxdlkRgdyqVndfJ2KcYY06Qs6IFtew/yxbf53DioGyFB9isxxvgXSzXg9cXbCQoQ\nxg3u5u1SjDGmybX5oD9YXsU7mTu4rHcnOsSEebscY4xpcm0+6N9btZOi8iomnmM3/jbG+Kc2HfSq\nyqzFWZzVJYb+3dp7uxxjjGkWbTroF/9QwHd7ipkwNBURu1WgMcY/temgn7koi/YRwVx5dmdvl2KM\nMc2mzQb9zn2lfLZxD2MGdiMsONDb5RhjTLNps0H/xpLtAIwfYl0qjTH+rU0GfVllNbOXZXPRGUkk\nt4/wdjnGGNOs2mTQf7gmlx9LKplko1QaY9qANhf0qsrMxVmkdYhiaI94b5djjDHNrs0F/crsfazf\neYAJ51iXSmNM29Dmgn7moiyiQ4O4pl/9294aY4x/alNBn3egjI/X7eK69GQiQz0ait8YY3xemwr6\nt5ZlU1WjTBia6u1SjDGmxbSZoK+oquHNpdmcf1oi3RMivV2OMca0mDYT9J9u2E1+Ubl1qTTGtDlt\nJuhnLcoiJT6C809L9HYpxhjTotpE0K/fuZ/M7T9y05AUAgKsS6Uxpm3xKOhFZKSIfCsiW0TkwQaW\ndxORhSKySkTWishlrvkjRGSFiKxzPQ5v6jfgiVmLswgPDuT69K7eeHljjPGqRvsYikgg8CIwAsgB\nlovIPFXd6LbaI8AcVf27iPQCPgZSgb3AFaqaKyJnAfOBFu3A/uPBCj5Yncu1A5KJDQ9uyZc2xphW\nwZMj+kHAFlX9QVUrgNnA6HrrKBDjmo4FcgFUdZWq5rrmbwDCRST05Mv23NuZOyivqmHCULtVoDGm\nbfIk6LsAO9ye53DkUfnjwHgRycE5mv95A/u5FlipquX1F4jIFBHJFJHM/Px8jwr3RHWN8vri7Qw5\nJY7TO8Y0voExxvihpjoZewPwmqomA5cBr4tI7b5F5Ezg98DtDW2sqtNUNV1V0xMTm65XzIJNe9i5\nr5SJdoGUMaYN8yTodwLuZzGTXfPc3QLMAVDVxUAYkAAgIsnAe8AEVd16sgUfj1mLs+gcG8aIXkkt\n+bLGGNOqeBL0y4E0EekuIiHAWGBevXWygQsBROQMnKDPF5F2wEfAg6r636Yru3Hf7yniv1sKGDck\nhaDANtGL1BhjGtRoAqpqFTAVp8fMJpzeNRtE5EkRudK12q+A20RkDZABTFJVdW13KvCoiKx2/XRo\nlndSz6zF2wkJCmDsQOtSaYxp2zwawlFVP8Y5yeo+71G36Y3AsAa2exp4+iRrPG4HyiqZuzKHK/p0\nJj6qRTv5GGNMq+OXbRpzV+RQUlHNxHOsS6Uxxvhd0NfUKLMWb6dft3b0SW7n7XKMMcbr/C7ov96y\nl217D1qXSmOMcfG7oJ+5KIuEqFAu693J26UYY0yr4FdBv73gIAu/zePGQV0JCfKrt2aMMSfMr9Lw\n9cXbCRRh3BA7CWuMMYf4TdCXVFQxJ3MHl5zVkaSYMG+XY4wxrYZH/eh9QVFZFeedlmi3CjTGmHr8\nJuiTYsJ44cb+3i7DGGNaHb9pujHGGNMwC3pjjPFzFvTGGOPnLOiNMcbPWdAbY4yfs6A3xhg/Z0Fv\njDF+zoLeGGP8nDh3/Gs9RCQf2H4Su0gA9jZROc3Nl2oF36rXl2oF36rXl2oF36r3ZGpNUdXEhha0\nuqA/WSKSqarp3q7DE75UK/hWvb5UK/hWvb5UK/hWvc1VqzXdGGOMn7OgN8YYP+ePQT/N2wUcB1+q\nFXyrXl+qFXyrXl+qFXyr3map1e/a6I0xxtTlj0f0xhhj3FjQG2OMn/OboBeRkSLyrYhsEZEHvV3P\nsYjIdBHJE5H13q6lMSLSVUQWishGEdkgIvd4u6ZjEZEwEVkmImtc9T7h7ZoaIyKBIrJKRP7l7Voa\nIyJZIrJORFaLSKa36zkWEWknIu+KyGYR2SQiQ71d09GISE/X7/TQzwERubfJ9u8PbfQiEgh8B4wA\ncoDlwA2qutGrhR2FiJwHFAOzVPUsb9dzLCLSCeikqitFJBpYAVzVin+3AkSqarGIBAPfAPeo6hIv\nl3ZUInIfkA7EqOoob9dzLCKSBaSraqu/AElEZgJfq+orIhICRKjqPm/X1RhXnu0EBqvqyVw8Wstf\njugHAVtU9QdVrQBmA6O9XNNRqepXQKG36/CEqu5S1ZWu6SJgE9DFu1UdnTqKXU+DXT+t9mhGRJKB\ny4FXvF2LPxGRWOA84FUAVa3whZB3uRDY2lQhD/4T9F2AHW7Pc2jFYeSrRCQV6Acs9W4lx+ZqClkN\n5AGfqWprrvfPwANAjbcL8ZAC/xaRFSIyxdvFHEN3IB+Y4WoWe0VEIr1dlIfGAhlNuUN/CXrTzEQk\nCpgL3KuqB7xdz7GoarWq9gWSgUEi0iqbx0RkFJCnqiu8XctxOFdV+wOXAne7miFboyCgP/B3Ve0H\nHARa9bk7AFcT05XAO025X38J+p1AV7fnya55pgm42rrnAm+q6v95ux5Pub6qLwRGeruWoxgGXOlq\n954NDBeRN7xb0rGp6k7XYx7wHk6zaWuUA+S4fZt7Fyf4W7tLgZWquqcpd+ovQb8cSBOR7q5PxLHA\nPC/X5BdcJzdfBTap6p+8XU9jRCRRRNq5psNxTtBv9m5VDVPVh1Q1WVVTcf5mP1fV8V4u66hEJNJ1\nQh5XM8jFQKvsOaaqu4EdItLTNetCoFV2IKjnBpq42Qacrzc+T1WrRGQqMB8IBKar6gYvl3VUIpIB\n/BRIEJEc4DFVfdW7VR3VMOAmYJ2r3Rvg/6nqx16s6Vg6ATNdPRcCgDmq2uq7LfqIJOA957OfIOAt\nVf3UuyUd08+BN10Hfz8Ak71czzG5PjxHALc3+b79oXulMcaYo/OXphtjjDFHYUFvjDF+zoLeGGP8\nnAW9Mcb4OQt6Y4zxcxb0pk0Skep6owU22VWTIpLqCyOTmrbDL/rRG3MCSl3DJBjj9+yI3hg3rvHW\n/9c15voyETnVNT9VRD4XkbUi8h8R6eaanyQi77nGv18jIue4dhUoIi+7xsT/t+sqXWO8woLetFXh\n9Zpuxrgt26+qvYEXcEaXBPgbMFNV+wBvAn91zf8r8KWqno0zlsqhK7LTgBdV9UxgH3BtM78fY47K\nrow1bZKIFKtqVAPzs4DhqvqDazC33aoaLyJ7cW7AUumav0tVE0QkH0hW1XK3faTiDI+c5nr+P0Cw\nqj7d/O/MmCPZEb0xR9KjTB+Pcrfpaux8mPEiC3pjjjTG7XGxa3oRzgiTAOOAr13T/wHuhNobnsS2\nVJHGeMqOMkxbFe42GifAp6p6qItlexFZi3NUfoNr3s9x7lb0a5w7Fx0aCfEeYJqI3IJz5H4nsKvZ\nqzfmOFgbvTFufOnm18Z4yppujDHGz9kRvTHG+Dk7ojfGGD9nQW+MMX7Ogt4YY/ycBb0xxvg5C3pj\njPFz/x86xo2+cV/2xwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GT2dRPM19Kok",
        "colab_type": "text"
      },
      "source": [
        "And confusion matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UWztNslAwmO",
        "colab_type": "code",
        "outputId": "c33afc3e-eae9-409c-a480-9cd5a2ff1c39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "pd.DataFrame(res['conf_matr'])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2436</td>\n",
              "      <td>228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>293</td>\n",
              "      <td>2767</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      0     1\n",
              "0  2436   228\n",
              "1   293  2767"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXjXyLARfRsM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}